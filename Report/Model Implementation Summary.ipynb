{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Program for Feedforward Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title            : Python Program for Feedforward Models\n",
    "Dataset          : CR (as an example) \n",
    "Feature Extration: Bag-of-Words\n",
    "Author           : Diardano Raihan\n",
    "\"\"\"\n",
    "\n",
    "##=========================Import Libraries======================#\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#==============================Step 1============================#\n",
    "# Load the dataset\n",
    "corpus = pd.read_pickle('../../0_data/CR/CR.pkl')\n",
    "\n",
    "#==============================Step 2============================#\n",
    "# Define functions needed for:\n",
    "# 1. Cleaning the text and turning into tokens\n",
    "# 2. Creating a vocabulary list for BoW\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "    \n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out stop words\n",
    "    tokens = [w for w in tokens if not w in stopwords]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) >= 1]\n",
    "    # Stem the token\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "vocab = Counter()\n",
    "\n",
    "def add_doc_to_vocab(docs, vocab):\n",
    "    '''\n",
    "    input:\n",
    "        docs: a list of sentences (docs)\n",
    "        vocab: a vocabulary dictionary\n",
    "    output:\n",
    "        return an updated vocabulary\n",
    "    '''\n",
    "    for doc in docs:\n",
    "        tokens = clean_doc(doc)\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "\n",
    "def doc_to_line(doc):\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [token for token in tokens if token in vocab]\n",
    "    line = ' '.join([token for token in tokens])\n",
    "    return line\n",
    "\n",
    "def clean_docs(docs):\n",
    "    lines = []\n",
    "    for doc in docs:\n",
    "        line = doc_to_line(doc)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def create_tokenizer(sentence):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# Define the feedforward model SNN-a\n",
    "def train_mlp_1(train_x, train_y, batch_size = 50, epochs = 10, \n",
    "                verbose =2):\n",
    "    \n",
    "    n_words = train_x.shape[1]\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(units=50, \n",
    "                              activation='relu', \n",
    "                              input_shape=(n_words,)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense( units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "    model.fit(train_x, train_y, batch_size, epochs, verbose)\n",
    "    return model\n",
    "\n",
    "# Define the feedforward model SNN-b\n",
    "def train_mlp_2(train_x, train_y, batch_size = 50, epochs = 10, \n",
    "                verbose =2):\n",
    "    \n",
    "    n_words = train_x.shape[1]\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense( units=100, activation='relu', \n",
    "                              input_shape=(n_words,)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense( units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "    model.fit(train_x, train_y, batch_size, epochs, verbose)\n",
    "    return model\n",
    "\n",
    "# Define the feedforward model SNN-b\n",
    "def train_mlp_3(train_x, train_y, batch_size = 50, epochs = 10, \n",
    "                verbose =2):\n",
    "    \n",
    "    n_words = train_x.shape[1]\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense( units=100, activation='relu', \n",
    "                              input_shape=(n_words,)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense( units=50, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense( units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "    model.fit(train_x, train_y, batch_size, epochs, verbose)\n",
    "    return model\n",
    "\n",
    "# Define the early stopping callbacks\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            min_delta=0, \n",
    "            patience=5, verbose=2, \n",
    "            mode='auto', \n",
    "            restore_best_weights=True)\n",
    "\n",
    "\n",
    "#==============================Step 4============================#\n",
    "# Begin the training process with BoW four word scoring options\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Run Experiment of 4 different modes of BoW word scoring\n",
    "modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "results_1 = pd.DataFrame()\n",
    "results_2 = pd.DataFrame()\n",
    "results_3 = pd.DataFrame()\n",
    "\n",
    "\n",
    "for mode in modes:\n",
    "    print('mode: ', mode)\n",
    "    acc_list_1 = []\n",
    "    acc_list_2 = []\n",
    "    acc_list_3 = []\n",
    "    \n",
    "    \n",
    "    # kfold.split() will return set indices for each split\n",
    "    for train, test in kfold.split(sentences):\n",
    "        # Instantiate a vocab object\n",
    "        vocab = Counter()\n",
    "\n",
    "        train_x, test_x = [], []\n",
    "        train_y, test_y = [], []\n",
    "\n",
    "        for i in train:\n",
    "            train_x.append(sentences[i])\n",
    "            train_y.append(labels[i])\n",
    "\n",
    "        for i in test:\n",
    "            test_x.append(sentences[i])\n",
    "            test_y.append(labels[i])\n",
    "\n",
    "        # Turn the labels into a numpy array\n",
    "        train_y = np.array(train_y)\n",
    "        test_y = np.array(test_y)\n",
    "\n",
    "        # Define a vocabulary for each fold\n",
    "        vocab = add_doc_to_vocab(train_x, vocab)\n",
    "        # print('The number of vocab: ', len(vocab))\n",
    "\n",
    "        # Clean the sentences\n",
    "        train_x = clean_docs(train_x, vocab)\n",
    "        test_x = clean_docs(test_x, vocab)\n",
    "\n",
    "        # encode data using freq mode\n",
    "        Xtrain, Xtest = prepare_data(train_x, test_x, mode)\n",
    "\n",
    "        # train the model\n",
    "        model_1 = train_mlp_1(Xtrain, train_y, Xtest, test_y, \n",
    "                              verbose=1)\n",
    "        model_2 = train_mlp_2(Xtrain, train_y, Xtest, test_y, \n",
    "                              verbose=1)\n",
    "        model_3 = train_mlp_3(Xtrain, train_y, Xtest, test_y, \n",
    "                              verbose=1)\n",
    "\n",
    "        # evaluate the model\n",
    "        loss_1, acc_1 = model_1.evaluate(Xtest, test_y, verbose=0)\n",
    "        loss_2, acc_2 = model_2.evaluate(Xtest, test_y, verbose=0)\n",
    "        loss_3, acc_3 = model_3.evaluate(Xtest, test_y, verbose=0)\n",
    "\n",
    "        acc_list_1.append(acc_1)\n",
    "        acc_list_2.append(acc_2)\n",
    "        acc_list_3.append(acc_3)\n",
    "        \n",
    "    results_1[mode] = acc_list_1\n",
    "    results_2[mode] = acc_list_2\n",
    "    results_3[mode] = acc_list_3\n",
    "\n",
    "print(results_1)\n",
    "print(results_2)\n",
    "print(results_3)\n",
    "\n",
    "#==============================Step 5============================#\n",
    "# Save the dataframe into excel file\n",
    "results_1.to_excel('BoW_SNN_a.xlsx', sheet_name='model_1')\n",
    "results_2.to_excel('BoW_SNN_b.xlsx', sheet_name='model_2')\n",
    "results_3.to_excel('BoW_SNN_c.xlsx', sheet_name='model_3')\n",
    "#================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title            : Python Program for Feedforward Models\n",
    "Dataset          : CR (as an example) \n",
    "Feature Extration: Average Word2Vec\n",
    "Author           : Diardano Raihan\n",
    "\"\"\"\n",
    "\n",
    "##=========================Import Libraries======================#\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#==============================Step 1============================#\n",
    "# Load the dataset\n",
    "corpus = pd.read_pickle('../../0_data/CR/CR.pkl')\n",
    "# Load the Word2Vec\n",
    "word2vec = KeyedVectors.load_word2vec_format(\n",
    "            '../GoogleNews-vectors-negative300.bin', \n",
    "            binary=True)\n",
    "\n",
    "#==============================Step 2============================#\n",
    "# Define functions needed to:\n",
    "# 1. Clean the text\n",
    "# 2. Convert the text into vectors based on Word2Vec\n",
    "\n",
    "def clean_doc(sentences, word_index):\n",
    "    clean_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower().split()\n",
    "        clean_word = []\n",
    "        for word in sentence:\n",
    "            if word in word_index:\n",
    "                clean_word.append(word)\n",
    "        clean_sentence = ' '.join(clean_word)\n",
    "        clean_sentences.append(clean_sentence)\n",
    "    return clean_sentences\n",
    "\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \n",
    "    # Split sentence into list of lower case words\n",
    "    words = (sentence.lower()).split()\n",
    "\n",
    "    # Initialize the average word vector\n",
    "    avg = np.zeros(word2vec.word_vec('i').shape)\n",
    "    \n",
    "    # Average the word vectors\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for w in words:\n",
    "        if w in word_to_vec_map:\n",
    "            total += word_to_vec_map.word_vec(w)\n",
    "            count += 1\n",
    "            \n",
    "    if count!= 0:\n",
    "        avg = total/count\n",
    "    else:\n",
    "        avg\n",
    "    return avg\n",
    "\n",
    "# Encode Sentence into Word2Vec Representation\n",
    "def encoded_sentences(sentences):\n",
    "\n",
    "    encoded_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        encoded_sentence = sentence_to_avg(sentence, word2vec)\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "    encoded_sentences = np.array(encoded_sentences)\n",
    "    return encoded_sentences\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# Model Definition\n",
    "\n",
    "# Define the feedforward model SNN-a\n",
    "def define_model(input_length=300):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(units=50, activation='relu', \n",
    "                              input_shape=(input_length,)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense( units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the feedforward model SNN-b\n",
    "def define_model_2(input_length=300):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(units=100, \n",
    "                              activation='relu', \n",
    "                              input_shape=(input_length,)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense( units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the feedforward model SNN-c\n",
    "def define_model_3(input_length=300):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(units=100, \n",
    "                              activation='relu', \n",
    "                              input_shape=(input_length,)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense( units=50, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense( units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the early stopping callbacks\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            min_delta=0, \n",
    "            patience=10, verbose=2, \n",
    "            mode='auto', \n",
    "            restore_best_weights=True)\n",
    "\n",
    "#==============================Step 4============================#\n",
    "# Begin the training process with Average Word2Vec\n",
    "\n",
    "# Parameter Initialization\n",
    "oov_tok = \"<UNK>\"\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', \n",
    "           'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "\n",
    "record_1 = pd.DataFrame(columns = columns)\n",
    "record_2 = pd.DataFrame(columns = columns)\n",
    "record_3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "exp=0\n",
    "acc_list_1 = []\n",
    "acc_list_2 = []\n",
    "acc_list_3 = []\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the data into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "    \n",
    "    # Define the word_index\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    # Clean the sentences\n",
    "    Xtrain = clean_doc(train_x, word_index)\n",
    "    Xtest = clean_doc(test_x, word_index)\n",
    "\n",
    "    # Encode the sentences into average Word2Vec representation\n",
    "    Xtrain = encoded_sentences(Xtrain)\n",
    "    Xtest = encoded_sentences(Xtest)\n",
    "    \n",
    "    # Define the input shape\n",
    "    model_1 = define_model_1(Xtrain.shape[1])\n",
    "    model_2 = define_model_2(Xtrain.shape[1])\n",
    "    model_3 = define_model_3(Xtrain.shape[1])\n",
    "\n",
    "    # Train the model\n",
    "    model_1.fit(Xtrain, train_y, batch_size=32, \n",
    "                epochs=40, verbose=1, \n",
    "                callbacks=[callbacks], \n",
    "                validation_data=(Xtest, test_y))\n",
    "\n",
    "    model_2.fit(Xtrain, train_y, batch_size=32, \n",
    "                epochs=40, verbose=1, \n",
    "                callbacks=[callbacks], \n",
    "                validation_data=(Xtest, test_y))\n",
    "    \n",
    "    model_3.fit(Xtrain, train_y, batch_size=32, \n",
    "                epochs=40, verbose=1, \n",
    "                callbacks=[callbacks], \n",
    "                validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss_1, acc_1 = model_1.evaluate(Xtest, test_y, verbose=0)\n",
    "    loss_2, acc_2 = model_2.evaluate(Xtest, test_y, verbose=0)\n",
    "    loss_3, acc_3 = model_3.evaluate(Xtest, test_y, verbose=0)\n",
    "\n",
    "\n",
    "    acc_list_1.append(acc_1)\n",
    "    acc_list_2.append(acc_2)\n",
    "    acc_list_3.append(acc_3)\n",
    "\n",
    "mean_acc_1 = np.array(acc_list_1).mean()\n",
    "mean_acc_2 = np.array(acc_list_2).mean()\n",
    "mean_acc_3 = np.array(acc_list_3).mean()\n",
    "\n",
    "entries_1 = acc_list_1 + [mean_acc_1]\n",
    "entries_2 = acc_list_2 + [mean_acc_2]\n",
    "entries_3 = acc_list_1 + [mean_acc_3]\n",
    "\n",
    "temp = pd.DataFrame([entries_1], columns=columns)\n",
    "record_1 = record_1.append(temp, ignore_index=True)\n",
    "temp = pd.DataFrame([entries_2], columns=columns)\n",
    "record_2 = record_2.append(temp, ignore_index=True)\n",
    "temp = pd.DataFrame([entries_3], columns=columns)\n",
    "record_3 = record_3.append(temp, ignore_index=True)\n",
    "\n",
    "print(record_1)\n",
    "print(record_2)\n",
    "print(record_3)\n",
    "\n",
    "#==============================Step 5============================#\n",
    "# Save the dataframe into excel file\n",
    "record_1.to_excel('WE_SNN_a.xlsx', sheet_name='model_1')\n",
    "record_2.to_excel('WE_SNN_b.xlsx', sheet_name='model_2')\n",
    "record_3.to_excel('WE_SNN_c.xlsx', sheet_name='model_3')\n",
    "#================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Program for CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title            : Python Program for CNN Models\n",
    "Dataset          : CR (as an example) \n",
    "Feature Extration: Random, Static, Dynamic Word2Vec\n",
    "Author           : Diardano Raihan\n",
    "\"\"\"\n",
    "\n",
    "##=========================Import Libraries======================#\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#==============================Step 1============================#\n",
    "# Load the dataset\n",
    "corpus = pd.read_pickle('../../0_data/CR/CR.pkl')\n",
    "# Load the Word2Vec\n",
    "word2vec = KeyedVectors.load_word2vec_format(\n",
    "            '../GoogleNews-vectors-negative300.bin', \n",
    "            binary=True)\n",
    "\n",
    "#==============================Step 2============================#\n",
    "# Defined all the functions needed as Text Preprocessing steps\n",
    "\n",
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length\n",
    "\n",
    "# Calculated the statistics of Word2Vec\n",
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "\n",
    "# Map the Word2Vec into a matrix for embedding weights\n",
    "def pretrained_embedding_matrix(word_to_vec_map, \n",
    "                                word_to_index, \n",
    "                                emb_mean, emb_std):\n",
    "    \n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution\n",
    "    embed_matrix = np.random.normal(emb_mean, \n",
    "                                    emb_std, \n",
    "                                    (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th \n",
    "    # word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            \n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# Model Definitions\n",
    "\n",
    "def define_model(filters = 100, kernel_size = 3, \n",
    "                 activation='relu', input_dim = None, \n",
    "                 output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Conv1D(filters=filters, \n",
    "                               kernel_size = kernel_size, \n",
    "                               activation = activation, \n",
    "                               # set 'axis' value to the first and \n",
    "                               # second axis of conv1D weights \n",
    "                               # (rows, cols)\n",
    "                               kernel_constraint = MaxNorm(\n",
    "                                   max_value=3, \n",
    "                                   axis=[0,1])),\n",
    "        \n",
    "        tf.keras.layers.MaxPool1D(2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation=activation, \n",
    "                              # set axis to 0 to constrain \n",
    "                              # each weight vector of length \n",
    "                              # (input_dim,) in dense layer\n",
    "                              kernel_constraint = MaxNorm(\n",
    "                                  max_value=3, axis=0)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def define_model_2(filters = 100, kernel_size = 3, \n",
    "                   activation='relu', input_dim = None, \n",
    "                   output_dim=300, max_length = None, \n",
    "                   emb_matrix = None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight \n",
    "                                  # with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not \n",
    "                                  # trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Conv1D(filters=filters, \n",
    "                               kernel_size = kernel_size, \n",
    "                               activation = activation, \n",
    "                               # set 'axis' value to the first and \n",
    "                               # second axis of conv1D weights \n",
    "                               # (rows, cols)\n",
    "                               kernel_constraint= MaxNorm(\n",
    "                                   max_value=3, axis=[0,1])),\n",
    "        \n",
    "        tf.keras.layers.MaxPool1D(2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation=activation, \n",
    "                              # set axis to 0 to constrain \n",
    "                              # each weight vector of length \n",
    "                              # (input_dim,) in dense layer\n",
    "                              kernel_constraint = MaxNorm(\n",
    "                                  max_value=3, axis=0)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_model_3(filters = 100, kernel_size = 3, \n",
    "                   activation='relu', input_dim = None, \n",
    "                   output_dim=300, max_length = None, \n",
    "                   emb_matrix = None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight \n",
    "                                  # with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not \n",
    "                                  # trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Conv1D(filters=filters, \n",
    "                               kernel_size = kernel_size, \n",
    "                               activation = activation, \n",
    "                               # set 'axis' value to the first and \n",
    "                               # second axis of conv1D weights \n",
    "                               # (rows, cols)\n",
    "                               kernel_constraint= MaxNorm(\n",
    "                                   max_value=3, axis=[0,1])),\n",
    "        \n",
    "        tf.keras.layers.MaxPool1D(2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation=activation, \n",
    "                              # set axis to 0 to constrain \n",
    "                              # each weight vector of length \n",
    "                              # (input_dim,) in dense layer\n",
    "                              kernel_constraint = MaxNorm(\n",
    "                                  max_value=3, axis=0)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define the early stopping callbacks\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            min_delta=0, \n",
    "            patience=10, verbose=2, \n",
    "            mode='auto', \n",
    "            restore_best_weights=True)\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# Begin the training process with random/static/dynamic Word2Vec\n",
    "\n",
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "activations = ['relu', 'tanh']\n",
    "filters = 100\n",
    "kernel_sizes = [1, 2, 3, 4, 5, 6]\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "\n",
    "columns = ['Activation', 'Filters', 'acc1', 'acc2', 'acc3', \n",
    "           'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', \n",
    "           'acc10', 'AVG']\n",
    "\n",
    "record_1 = pd.DataFrame(columns = columns)\n",
    "record_2 = pd.DataFrame(columns = columns)\n",
    "record_3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "for activation in activations:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # kfold.split() will return set indices for each split\n",
    "        acc_list_1 = []\n",
    "        acc_list_2 = []\n",
    "        acc_list_3 = []\n",
    "        \n",
    "        for train, test in kfold.split(sentences):\n",
    "            \n",
    "            train_x, test_x = [], []\n",
    "            train_y, test_y = [], []\n",
    "            \n",
    "            for i in train:\n",
    "                train_x.append(sentences[i])\n",
    "                train_y.append(labels[i])\n",
    "\n",
    "            for i in test:\n",
    "                test_x.append(sentences[i])\n",
    "                test_y.append(labels[i])\n",
    "\n",
    "            # Turn the labels into a numpy array\n",
    "            train_y = np.array(train_y)\n",
    "            test_y = np.array(test_y)\n",
    "\n",
    "            # encode data using\n",
    "            # Cleaning and Tokenization\n",
    "            tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "            tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "            # Turn the text into sequence\n",
    "            training_sequences = tokenizer.texts_to_sequences(\n",
    "                                 train_x)\n",
    "            test_sequences = tokenizer.texts_to_sequences(\n",
    "                             test_x)\n",
    "\n",
    "            max_len = max_length(training_sequences)\n",
    "\n",
    "            # Pad the sequence to have the same size\n",
    "            Xtrain = pad_sequences(training_sequences, \n",
    "                                   maxlen=max_len, \n",
    "                                   padding=padding_type, \n",
    "                                   truncating=trunc_type)\n",
    "            Xtest = pad_sequences(test_sequences, \n",
    "                                  maxlen=max_len, \n",
    "                                  padding=padding_type, \n",
    "                                  truncating=trunc_type)\n",
    "\n",
    "            word_index = tokenizer.word_index\n",
    "            vocab_size = len(word_index)+1\n",
    "            \n",
    "            \n",
    "            emb_matrix = pretrained_embedding_matrix(word2vec, \n",
    "                                                     word_index, \n",
    "                                                     emb_mean, \n",
    "                                                     emb_std)\n",
    "            \n",
    "            # Define the models to train\n",
    "            model_1 = define_model(filters, \n",
    "                                   kernel_size, \n",
    "                                   activation, \n",
    "                                   input_dim=vocab_size, \n",
    "                                   max_length=max_len)\n",
    "            \n",
    "            model_2 = define_model_2(filters, \n",
    "                                     kernel_size, \n",
    "                                     activation, \n",
    "                                     input_dim=vocab_size, \n",
    "                                     max_length=max_len, \n",
    "                                     emb_matrix=emb_matrix)\n",
    "            \n",
    "            model_3 = define_model_3(filters, \n",
    "                                     kernel_size, \n",
    "                                     activation, \n",
    "                                     input_dim=vocab_size, \n",
    "                                     max_length=max_len, \n",
    "                                     emb_matrix=emb_matrix)\n",
    "            \n",
    "\n",
    "            # Train the models\n",
    "            model_1.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1, \n",
    "                        callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "\n",
    "            model_2.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1,\n",
    "                        callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "            \n",
    "            model_3.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1, \n",
    "                      callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "\n",
    "            # evaluate the model\n",
    "            loss_1, acc_1 = model_1.evaluate(Xtest, test_y)\n",
    "            loss_2, acc_2 = model_2.evaluate(Xtest, test_y)\n",
    "            loss_3, acc_3 = model_3.evaluate(Xtest, test_y)\n",
    "\n",
    "            acc_list_1.append(acc*100)\n",
    "            acc_list_2.append(acc*100)\n",
    "            acc_list_3.append(acc*100)\n",
    "            \n",
    "        mean_acc_1 = np.array(acc_list_1).mean()\n",
    "        mean_acc_2 = np.array(acc_list_2).mean()\n",
    "        mean_acc_3 = np.array(acc_list_3).mean()\n",
    "        \n",
    "        parameters = [activation, kernel_size]\n",
    "        entries_1 = parameters + acc_list + [mean_acc_1]\n",
    "        entries_2 = parameters + acc_list + [mean_acc_2]\n",
    "        entries_3 = parameters + acc_list + [mean_acc_3]\n",
    "        \n",
    "        temp = pd.DataFrame([entries_1], columns=columns)\n",
    "        record_1 = record_1.append(temp, ignore_index=True)\n",
    "        temp = pd.DataFrame([entries_2], columns=columns)\n",
    "        record_2 = record_2.append(temp, ignore_index=True)\n",
    "        temp = pd.DataFrame([entries_3], columns=columns)\n",
    "        record_3 = record_3.append(temp, ignore_index=True)\n",
    "\n",
    "        print(record_1)\n",
    "        print(record_2)\n",
    "        print(record_3)\n",
    "\n",
    "#==============================Step 5============================#\n",
    "# Save the dataframe into excel file\n",
    "record_1.to_excel('WE_CNN_1.xlsx', sheet_name='random')\n",
    "record_2.to_excel('WE_CNN_2.xlsx', sheet_name='static')\n",
    "record_3.to_excel('WE_CNN_3.xlsx', sheet_name='dynamic')\n",
    "#================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Program for TCN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title            : Python Program for TCN Models\n",
    "Dataset          : CR (as an example) \n",
    "Feature Extration: Random, Static, Dynamic Word2Vec\n",
    "Author           : Diardano Raihan (inspired by: christofhenkel)\n",
    "https://www.kaggle.com/christofhenkel/temporal-convolutional-network\n",
    "\"\"\"\n",
    "\n",
    "##=========================Import Libraries======================#\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from tensorflow.keras.layers import Input, Embedding\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#==============================Step 1============================#\n",
    "# Load the dataset\n",
    "corpus = pd.read_pickle('../../0_data/CR/CR.pkl')\n",
    "# Load the Word2Vec\n",
    "word2vec = KeyedVectors.load_word2vec_format(\n",
    "            '../GoogleNews-vectors-negative300.bin', \n",
    "            binary=True)\n",
    "\n",
    "#==============================Step 2============================#\n",
    "# Defined all the functions needed as Text Preprocessing steps\n",
    "\n",
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length\n",
    "\n",
    "# Calculated the statistics of Word2Vec\n",
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "\n",
    "# Map the Word2Vec into a matrix for embedding weights\n",
    "def pretrained_embedding_matrix(word_to_vec_map, \n",
    "                                word_to_index, \n",
    "                                emb_mean, emb_std):\n",
    "    \n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution\n",
    "    embed_matrix = np.random.normal(emb_mean, \n",
    "                                    emb_std, \n",
    "                                    (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th \n",
    "    # word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            \n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# TCN-rand\n",
    "def define_model(kernel_size = 3, activation='relu', \n",
    "                 input_dim = None, output_dim=300, \n",
    "                 max_length = None ):\n",
    "    \n",
    "    inp = Input( shape=(max_length,))\n",
    "    x = Embedding(input_dim=input_dim, \n",
    "                  output_dim=output_dim, \n",
    "                  input_length=max_length)(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = TCN(128,dilations = [1, 2, 4],\n",
    "            kernel_size = kernel_size,\n",
    "            return_sequences=True, \n",
    "            activation = activation, \n",
    "            name = 'tcn1')(x)\n",
    "    \n",
    "    x = TCN(64,dilations = [1, 2, 4], \n",
    "            kernel_size = kernel_size,\n",
    "            return_sequences=True, \n",
    "            activation = activation, \n",
    "            name = 'tcn2')(x)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    conc = Dense(16, activation=\"relu\")(conc)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)    \n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# TCN-static\n",
    "def define_model_2(kernel_size = 3, activation='relu', \n",
    "                   input_dim = None, \n",
    "                   output_dim=300, max_length = None, \n",
    "                   emb_matrix = None):\n",
    "    \n",
    "    inp = Input( shape=(max_length,))\n",
    "    x = Embedding(input_dim=input_dim, \n",
    "                  output_dim=output_dim, \n",
    "                  input_length=max_length,\n",
    "                  # Assign the embedding weight with \n",
    "                  # word2vec embedding marix\n",
    "                  weights = [emb_matrix],\n",
    "                  # Set the weight to be not trainable \n",
    "                  # (static)\n",
    "                  trainable = False)(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = TCN(128,dilations = [1, 2, 4],\n",
    "            kernel_size = kernel_size,\n",
    "            return_sequences=True, \n",
    "            activation = activation, \n",
    "            name = 'tcn1')(x)\n",
    "    x = TCN(64,dilations = [1, 2, 4],\n",
    "            kernel_size = kernel_size,\n",
    "            return_sequences=True, \n",
    "            activation = activation, \n",
    "            name = 'tcn2')(x)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    conc = Dense(16, activation=\"relu\")(conc)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)    \n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile( loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# TCN-dynamic\n",
    "def define_model_3(kernel_size = 3, activation='relu', \n",
    "                   input_dim = None, \n",
    "                   output_dim=300, max_length = None, \n",
    "                   emb_matrix = None):\n",
    "    \n",
    "    inp = Input( shape=(max_length,))\n",
    "    x = Embedding(input_dim=input_dim, \n",
    "                  output_dim=output_dim, \n",
    "                  input_length=max_length,\n",
    "                  # Assign the embedding weight with \n",
    "                  # word2vec embedding marix\n",
    "                  weights = [emb_matrix],\n",
    "                  # Set the weight to be not trainable \n",
    "                  # (static)\n",
    "                  trainable = True)(inp)\n",
    "    \n",
    "    x = SpatialDropout1D(0.1)(x)\n",
    "    \n",
    "    x = TCN(128,dilations = [1, 2, 4],\n",
    "            kernel_size = kernel_size,\n",
    "            return_sequences=True, \n",
    "            activation = activation, \n",
    "            name = 'tcn1')(x)\n",
    "    x = TCN(64,dilations = [1, 2, 4],\n",
    "            kernel_size = kernel_size,\n",
    "            return_sequences=True, \n",
    "            activation = activation, \n",
    "            name = 'tcn2')(x)\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    conc = Dense(16, activation=\"relu\")(conc)\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)    \n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile( loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the early stopping callbacks\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            min_delta=0, \n",
    "            patience=10, verbose=2, \n",
    "            mode='auto', \n",
    "            restore_best_weights=True)\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# Begin the training process with random/static/dynamic Word2Vec\n",
    "\n",
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "activations = ['relu', 'tanh']\n",
    "filters = 100\n",
    "kernel_sizes = [1, 2, 3, 4, 5, 6]\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "\n",
    "columns = ['Activation', 'Filters', 'acc1', 'acc2', 'acc3', \n",
    "           'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', \n",
    "           'acc10', 'AVG']\n",
    "\n",
    "record_1 = pd.DataFrame(columns = columns)\n",
    "record_2 = pd.DataFrame(columns = columns)\n",
    "record_3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "for activation in activations:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # kfold.split() will return set indices for each split\n",
    "        acc_list_1 = []\n",
    "        acc_list_2 = []\n",
    "        acc_list_3 = []\n",
    "        \n",
    "        for train, test in kfold.split(sentences):\n",
    "            \n",
    "            train_x, test_x = [], []\n",
    "            train_y, test_y = [], []\n",
    "            \n",
    "            for i in train:\n",
    "                train_x.append(sentences[i])\n",
    "                train_y.append(labels[i])\n",
    "\n",
    "            for i in test:\n",
    "                test_x.append(sentences[i])\n",
    "                test_y.append(labels[i])\n",
    "\n",
    "            # Turn the labels into a numpy array\n",
    "            train_y = np.array(train_y)\n",
    "            test_y = np.array(test_y)\n",
    "\n",
    "            # encode data using\n",
    "            # Cleaning and Tokenization\n",
    "            tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "            tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "            # Turn the text into sequence\n",
    "            training_sequences = tokenizer.texts_to_sequences(\n",
    "                                 train_x)\n",
    "            test_sequences = tokenizer.texts_to_sequences(\n",
    "                             test_x)\n",
    "\n",
    "            max_len = max_length(training_sequences)\n",
    "\n",
    "            # Pad the sequence to have the same size\n",
    "            Xtrain = pad_sequences(training_sequences, \n",
    "                                   maxlen=max_len, \n",
    "                                   padding=padding_type, \n",
    "                                   truncating=trunc_type)\n",
    "            Xtest = pad_sequences(test_sequences, \n",
    "                                  maxlen=max_len, \n",
    "                                  padding=padding_type, \n",
    "                                  truncating=trunc_type)\n",
    "\n",
    "            word_index = tokenizer.word_index\n",
    "            vocab_size = len(word_index)+1\n",
    "            \n",
    "            \n",
    "            emb_matrix = pretrained_embedding_matrix(word2vec, \n",
    "                                                     word_index, \n",
    "                                                     emb_mean, \n",
    "                                                     emb_std)\n",
    "            \n",
    "            # Define the models to train\n",
    "            model_1 = define_model(filters, \n",
    "                                   kernel_size, \n",
    "                                   activation, \n",
    "                                   input_dim=vocab_size, \n",
    "                                   max_length=max_len)\n",
    "            \n",
    "            model_2 = define_model_2(filters, \n",
    "                                     kernel_size, \n",
    "                                     activation, \n",
    "                                     input_dim=vocab_size, \n",
    "                                     max_length=max_len, \n",
    "                                     emb_matrix=emb_matrix)\n",
    "            \n",
    "            model_3 = define_model_3(filters, \n",
    "                                     kernel_size, \n",
    "                                     activation, \n",
    "                                     input_dim=vocab_size, \n",
    "                                     max_length=max_len, \n",
    "                                     emb_matrix=emb_matrix)\n",
    "            \n",
    "\n",
    "            # Train the models\n",
    "            model_1.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1, \n",
    "                        callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "\n",
    "            model_2.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1,\n",
    "                        callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "            \n",
    "            model_3.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1, \n",
    "                      callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "\n",
    "            # evaluate the model\n",
    "            loss_1, acc_1 = model_1.evaluate(Xtest, test_y)\n",
    "            loss_2, acc_2 = model_2.evaluate(Xtest, test_y)\n",
    "            loss_3, acc_3 = model_3.evaluate(Xtest, test_y)\n",
    "\n",
    "            acc_list_1.append(acc*100)\n",
    "            acc_list_2.append(acc*100)\n",
    "            acc_list_3.append(acc*100)\n",
    "            \n",
    "        mean_acc_1 = np.array(acc_list_1).mean()\n",
    "        mean_acc_2 = np.array(acc_list_2).mean()\n",
    "        mean_acc_3 = np.array(acc_list_3).mean()\n",
    "        \n",
    "        parameters = [activation, kernel_size]\n",
    "        entries_1 = parameters + acc_list + [mean_acc_1]\n",
    "        entries_2 = parameters + acc_list + [mean_acc_2]\n",
    "        entries_3 = parameters + acc_list + [mean_acc_3]\n",
    "        \n",
    "        temp = pd.DataFrame([entries_1], columns=columns)\n",
    "        record_1 = record_1.append(temp, ignore_index=True)\n",
    "        temp = pd.DataFrame([entries_2], columns=columns)\n",
    "        record_2 = record_2.append(temp, ignore_index=True)\n",
    "        temp = pd.DataFrame([entries_3], columns=columns)\n",
    "        record_3 = record_3.append(temp, ignore_index=True)\n",
    "\n",
    "        print(record_1)\n",
    "        print(record_2)\n",
    "        print(record_3)\n",
    "\n",
    "#==============================Step 5============================#\n",
    "# Save the dataframe into excel file\n",
    "record_1.to_excel('WE_CNN_1.xlsx', sheet_name='random')\n",
    "record_2.to_excel('WE_CNN_2.xlsx', sheet_name='static')\n",
    "record_3.to_excel('WE_CNN_3.xlsx', sheet_name='dynamic')\n",
    "#================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Program for BiGRU/LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title            : Python Program for BiGRU/LSTM Models\n",
    "Dataset          : CR (as an example)\n",
    "Feature Extration: Random, Static, Dynamic Word2Vec\n",
    "Author           : Diardano Raihan\n",
    "\"\"\"\n",
    "\n",
    "##=========================Import Libraries======================#\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#==============================Step 1============================#\n",
    "# Load the dataset\n",
    "corpus = pd.read_pickle('../../0_data/CR/CR.pkl')\n",
    "# Load the Word2Vec\n",
    "word2vec = KeyedVectors.load_word2vec_format(\n",
    "            '../GoogleNews-vectors-negative300.bin', \n",
    "            binary=True)\n",
    "\n",
    "#==============================Step 2============================#\n",
    "# Defined all the functions needed as Text Preprocessing steps\n",
    "\n",
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length\n",
    "\n",
    "# Calculated the statistics of Word2Vec\n",
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "\n",
    "# Map the Word2Vec into a matrix for embedding weights\n",
    "def pretrained_embedding_matrix(word_to_vec_map, \n",
    "                                word_to_index, \n",
    "                                emb_mean, emb_std):\n",
    "    \n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution\n",
    "    embed_matrix = np.random.normal(emb_mean, \n",
    "                                    emb_std, \n",
    "                                    (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th \n",
    "    # word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            \n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# Model Definitions\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, \n",
    "                 output_dim=300, \n",
    "                 max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        # Change it to tf.keras.layers.LSTM(64) for LSTM version\n",
    "        # inside the Bidirectional layer\n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, \n",
    "                   output_dim=300, \n",
    "                   max_length = None, \n",
    "                   emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight \n",
    "                                  # with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not \n",
    "                                  # trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        # Change it to tf.keras.layers.LSTM(64) for LSTM version\n",
    "        # inside the Bidirectional layer        \n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_model_3(input_dim = None, \n",
    "                   output_dim=300, \n",
    "                   max_length = None, \n",
    "                   emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight \n",
    "                                  # with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not \n",
    "                                  # trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        # Change it to tf.keras.layers.LSTM(64) for LSTM version\n",
    "        # inside the Bidirectional layer        \n",
    "        tf.keras.layers.Bidirectional((tf.keras.layers.GRU(64))),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the early stopping callbacks\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            min_delta=0, \n",
    "            patience=10, verbose=2, \n",
    "            mode='auto', \n",
    "            restore_best_weights=True)\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# Begin the training process with random/static/dynamic Word2Vec\n",
    "\n",
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "activations = ['relu', 'tanh']\n",
    "filters = 100\n",
    "kernel_sizes = [1, 2, 3, 4, 5, 6]\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "\n",
    "columns = ['Activation', 'Filters', 'acc1', 'acc2', 'acc3', \n",
    "           'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', \n",
    "           'acc10', 'AVG']\n",
    "\n",
    "record_1 = pd.DataFrame(columns = columns)\n",
    "record_2 = pd.DataFrame(columns = columns)\n",
    "record_3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "for activation in activations:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # kfold.split() will return set indices for each split\n",
    "        acc_list_1 = []\n",
    "        acc_list_2 = []\n",
    "        acc_list_3 = []\n",
    "        \n",
    "        for train, test in kfold.split(sentences):\n",
    "            \n",
    "            train_x, test_x = [], []\n",
    "            train_y, test_y = [], []\n",
    "            \n",
    "            for i in train:\n",
    "                train_x.append(sentences[i])\n",
    "                train_y.append(labels[i])\n",
    "\n",
    "            for i in test:\n",
    "                test_x.append(sentences[i])\n",
    "                test_y.append(labels[i])\n",
    "\n",
    "            # Turn the labels into a numpy array\n",
    "            train_y = np.array(train_y)\n",
    "            test_y = np.array(test_y)\n",
    "\n",
    "            # encode data using\n",
    "            # Cleaning and Tokenization\n",
    "            tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "            tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "            # Turn the text into sequence\n",
    "            training_sequences = tokenizer.texts_to_sequences(\n",
    "                                 train_x)\n",
    "            test_sequences = tokenizer.texts_to_sequences(\n",
    "                             test_x)\n",
    "\n",
    "            max_len = max_length(training_sequences)\n",
    "\n",
    "            # Pad the sequence to have the same size\n",
    "            Xtrain = pad_sequences(training_sequences, \n",
    "                                   maxlen=max_len, \n",
    "                                   padding=padding_type, \n",
    "                                   truncating=trunc_type)\n",
    "            Xtest = pad_sequences(test_sequences, \n",
    "                                  maxlen=max_len, \n",
    "                                  padding=padding_type, \n",
    "                                  truncating=trunc_type)\n",
    "\n",
    "            word_index = tokenizer.word_index\n",
    "            vocab_size = len(word_index)+1\n",
    "            \n",
    "            \n",
    "            emb_matrix = pretrained_embedding_matrix(word2vec, \n",
    "                                                     word_index, \n",
    "                                                     emb_mean, \n",
    "                                                     emb_std)\n",
    "            \n",
    "            # Define the models to train\n",
    "            model_1 = define_model(filters, \n",
    "                                   kernel_size, \n",
    "                                   activation, \n",
    "                                   input_dim=vocab_size, \n",
    "                                   max_length=max_len)\n",
    "            \n",
    "            model_2 = define_model_2(filters, \n",
    "                                     kernel_size, \n",
    "                                     activation, \n",
    "                                     input_dim=vocab_size, \n",
    "                                     max_length=max_len, \n",
    "                                     emb_matrix=emb_matrix)\n",
    "            \n",
    "            model_3 = define_model_3(filters, \n",
    "                                     kernel_size, \n",
    "                                     activation, \n",
    "                                     input_dim=vocab_size, \n",
    "                                     max_length=max_len, \n",
    "                                     emb_matrix=emb_matrix)\n",
    "            \n",
    "\n",
    "            # Train the models\n",
    "            model_1.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1, \n",
    "                        callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "\n",
    "            model_2.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1,\n",
    "                        callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "            \n",
    "            model_3.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1, \n",
    "                      callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "\n",
    "            # evaluate the model\n",
    "            loss_1, acc_1 = model_1.evaluate(Xtest, test_y)\n",
    "            loss_2, acc_2 = model_2.evaluate(Xtest, test_y)\n",
    "            loss_3, acc_3 = model_3.evaluate(Xtest, test_y)\n",
    "\n",
    "            acc_list_1.append(acc*100)\n",
    "            acc_list_2.append(acc*100)\n",
    "            acc_list_3.append(acc*100)\n",
    "            \n",
    "        mean_acc_1 = np.array(acc_list_1).mean()\n",
    "        mean_acc_2 = np.array(acc_list_2).mean()\n",
    "        mean_acc_3 = np.array(acc_list_3).mean()\n",
    "        \n",
    "        parameters = [activation, kernel_size]\n",
    "        entries_1 = parameters + acc_list + [mean_acc_1]\n",
    "        entries_2 = parameters + acc_list + [mean_acc_2]\n",
    "        entries_3 = parameters + acc_list + [mean_acc_3]\n",
    "        \n",
    "        temp = pd.DataFrame([entries_1], columns=columns)\n",
    "        record_1 = record_1.append(temp, ignore_index=True)\n",
    "        temp = pd.DataFrame([entries_2], columns=columns)\n",
    "        record_2 = record_2.append(temp, ignore_index=True)\n",
    "        temp = pd.DataFrame([entries_3], columns=columns)\n",
    "        record_3 = record_3.append(temp, ignore_index=True)\n",
    "\n",
    "        print(record_1)\n",
    "        print(record_2)\n",
    "        print(record_3)\n",
    "\n",
    "#==============================Step 5============================#\n",
    "# Save the dataframe into excel file\n",
    "record_1.to_excel('WE_CNN_1.xlsx', sheet_name='random')\n",
    "record_2.to_excel('WE_CNN_2.xlsx', sheet_name='static')\n",
    "record_3.to_excel('WE_CNN_3.xlsx', sheet_name='dynamic')\n",
    "#================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Program for Stacked BiGRU/LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title            : Python Program for Stacked BiGRU/LSTM Models\n",
    "Dataset          : CR (as an example)\n",
    "Feature Extration: Random, Static, Dynamic Word2Vec\n",
    "Author           : Diardano Raihan\n",
    "\"\"\"\n",
    "\n",
    "##=========================Import Libraries======================#\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#==============================Step 1============================#\n",
    "# Load the dataset\n",
    "corpus = pd.read_pickle('../../0_data/CR/CR.pkl')\n",
    "# Load the Word2Vec\n",
    "word2vec = KeyedVectors.load_word2vec_format(\n",
    "            '../GoogleNews-vectors-negative300.bin', \n",
    "            binary=True)\n",
    "\n",
    "#==============================Step 2============================#\n",
    "# Defined all the functions needed as Text Preprocessing steps\n",
    "\n",
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length\n",
    "\n",
    "# Calculated the statistics of Word2Vec\n",
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "\n",
    "# Map the Word2Vec into a matrix for embedding weights\n",
    "def pretrained_embedding_matrix(word_to_vec_map, \n",
    "                                word_to_index, \n",
    "                                emb_mean, emb_std):\n",
    "    \n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution\n",
    "    embed_matrix = np.random.normal(emb_mean, \n",
    "                                    emb_std, \n",
    "                                    (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th \n",
    "    # word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            \n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# Model Definitions\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, \n",
    "                 output_dim=300, \n",
    "                 max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        # Change it to tf.keras.layers.LSTM(64) for LSTM version\n",
    "        # inside the Bidirectional layer\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
    "                                      64, \n",
    "                                      return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
    "                                      64, \n",
    "                                      return_sequences=False)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, \n",
    "                   output_dim=300, \n",
    "                   max_length = None, \n",
    "                   emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight \n",
    "                                  # with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not \n",
    "                                  # trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        # Change it to tf.keras.layers.LSTM(64) for LSTM version\n",
    "        # inside the Bidirectional layer\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
    "                                      64, \n",
    "                                      return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
    "                                      64, \n",
    "                                      return_sequences=False)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def define_model_3(input_dim = None, \n",
    "                   output_dim=300, \n",
    "                   max_length = None, \n",
    "                   emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight \n",
    "                                  # with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not \n",
    "                                  # trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        # Change it to tf.keras.layers.LSTM(64) for LSTM version\n",
    "        # inside the Bidirectional layer\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
    "                                      64, \n",
    "                                      return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
    "                                      64, \n",
    "                                      return_sequences=False)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # Propagate X through a Dense layer with 1 unit\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', \n",
    "                  optimizer = 'adam', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the early stopping callbacks\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            min_delta=0, \n",
    "            patience=10, verbose=2, \n",
    "            mode='auto', \n",
    "            restore_best_weights=True)\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# Begin the training process with random/static/dynamic Word2Vec\n",
    "\n",
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "activations = ['relu', 'tanh']\n",
    "filters = 100\n",
    "kernel_sizes = [1, 2, 3, 4, 5, 6]\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "\n",
    "columns = ['Activation', 'Filters', 'acc1', 'acc2', 'acc3', \n",
    "           'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', \n",
    "           'acc10', 'AVG']\n",
    "\n",
    "record_1 = pd.DataFrame(columns = columns)\n",
    "record_2 = pd.DataFrame(columns = columns)\n",
    "record_3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "for activation in activations:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # kfold.split() will return set indices for each split\n",
    "        acc_list_1 = []\n",
    "        acc_list_2 = []\n",
    "        acc_list_3 = []\n",
    "        \n",
    "        for train, test in kfold.split(sentences):\n",
    "            \n",
    "            train_x, test_x = [], []\n",
    "            train_y, test_y = [], []\n",
    "            \n",
    "            for i in train:\n",
    "                train_x.append(sentences[i])\n",
    "                train_y.append(labels[i])\n",
    "\n",
    "            for i in test:\n",
    "                test_x.append(sentences[i])\n",
    "                test_y.append(labels[i])\n",
    "\n",
    "            # Turn the labels into a numpy array\n",
    "            train_y = np.array(train_y)\n",
    "            test_y = np.array(test_y)\n",
    "\n",
    "            # encode data using\n",
    "            # Cleaning and Tokenization\n",
    "            tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "            tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "            # Turn the text into sequence\n",
    "            training_sequences = tokenizer.texts_to_sequences(\n",
    "                                 train_x)\n",
    "            test_sequences = tokenizer.texts_to_sequences(\n",
    "                             test_x)\n",
    "\n",
    "            max_len = max_length(training_sequences)\n",
    "\n",
    "            # Pad the sequence to have the same size\n",
    "            Xtrain = pad_sequences(training_sequences, \n",
    "                                   maxlen=max_len, \n",
    "                                   padding=padding_type, \n",
    "                                   truncating=trunc_type)\n",
    "            Xtest = pad_sequences(test_sequences, \n",
    "                                  maxlen=max_len, \n",
    "                                  padding=padding_type, \n",
    "                                  truncating=trunc_type)\n",
    "\n",
    "            word_index = tokenizer.word_index\n",
    "            vocab_size = len(word_index)+1\n",
    "            \n",
    "            \n",
    "            emb_matrix = pretrained_embedding_matrix(word2vec, \n",
    "                                                     word_index, \n",
    "                                                     emb_mean, \n",
    "                                                     emb_std)\n",
    "            \n",
    "            # Define the models to train\n",
    "            model_1 = define_model(filters, \n",
    "                                   kernel_size, \n",
    "                                   activation, \n",
    "                                   input_dim=vocab_size, \n",
    "                                   max_length=max_len)\n",
    "            \n",
    "            model_2 = define_model_2(filters, \n",
    "                                     kernel_size, \n",
    "                                     activation, \n",
    "                                     input_dim=vocab_size, \n",
    "                                     max_length=max_len, \n",
    "                                     emb_matrix=emb_matrix)\n",
    "            \n",
    "            model_3 = define_model_3(filters, \n",
    "                                     kernel_size, \n",
    "                                     activation, \n",
    "                                     input_dim=vocab_size, \n",
    "                                     max_length=max_len, \n",
    "                                     emb_matrix=emb_matrix)\n",
    "            \n",
    "\n",
    "            # Train the models\n",
    "            model_1.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1, \n",
    "                        callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "\n",
    "            model_2.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1,\n",
    "                        callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "            \n",
    "            model_3.fit(Xtrain, train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1, \n",
    "                      callbacks=[callbacks], \n",
    "                        validation_data=(Xtest, test_y))\n",
    "\n",
    "            # evaluate the model\n",
    "            loss_1, acc_1 = model_1.evaluate(Xtest, test_y)\n",
    "            loss_2, acc_2 = model_2.evaluate(Xtest, test_y)\n",
    "            loss_3, acc_3 = model_3.evaluate(Xtest, test_y)\n",
    "\n",
    "            acc_list_1.append(acc*100)\n",
    "            acc_list_2.append(acc*100)\n",
    "            acc_list_3.append(acc*100)\n",
    "            \n",
    "        mean_acc_1 = np.array(acc_list_1).mean()\n",
    "        mean_acc_2 = np.array(acc_list_2).mean()\n",
    "        mean_acc_3 = np.array(acc_list_3).mean()\n",
    "        \n",
    "        parameters = [activation, kernel_size]\n",
    "        entries_1 = parameters + acc_list + [mean_acc_1]\n",
    "        entries_2 = parameters + acc_list + [mean_acc_2]\n",
    "        entries_3 = parameters + acc_list + [mean_acc_3]\n",
    "        \n",
    "        temp = pd.DataFrame([entries_1], columns=columns)\n",
    "        record_1 = record_1.append(temp, ignore_index=True)\n",
    "        temp = pd.DataFrame([entries_2], columns=columns)\n",
    "        record_2 = record_2.append(temp, ignore_index=True)\n",
    "        temp = pd.DataFrame([entries_3], columns=columns)\n",
    "        record_3 = record_3.append(temp, ignore_index=True)\n",
    "\n",
    "        print(record_1)\n",
    "        print(record_2)\n",
    "        print(record_3)\n",
    "\n",
    "#==============================Step 5============================#\n",
    "# Save the dataframe into excel file\n",
    "record_1.to_excel('WE_StackedBiGRU/LSTM_1.xlsx', sheet_name='random')\n",
    "record_2.to_excel('WE_StackedBiGRU/LSTM_2.xlsx', sheet_name='static')\n",
    "record_3.to_excel('WE_StackedBiGRU/LSTM_3.xlsx', sheet_name='dynamic')\n",
    "#================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Program for Ensemble Learning-based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title            : Python Program for Ensemble-based Models\n",
    "Dataset          : CR (as an example)\n",
    "Feature Extration: Random, Static, Dynamic Word2Vec\n",
    "Author           : Diardano Raihan\n",
    "\"\"\"\n",
    "\n",
    "##=========================Import Libraries======================#\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D\n",
    "from tensorflow.keras.layers import Dropout, MaxPool1D, Flatten\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, GRU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "#==============================Step 1============================#\n",
    "# Load the dataset\n",
    "corpus = pd.read_pickle('../../0_data/CR/CR.pkl')\n",
    "# Load the Word2Vec\n",
    "word2vec = KeyedVectors.load_word2vec_format(\n",
    "            '../GoogleNews-vectors-negative300.bin', \n",
    "            binary=True)\n",
    "\n",
    "#==============================Step 2============================#\n",
    "# Defined all the functions needed as Text Preprocessing steps\n",
    "\n",
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length\n",
    "\n",
    "# Calculated the statistics of Word2Vec\n",
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "\n",
    "# Map the Word2Vec into a matrix for embedding weights\n",
    "def pretrained_embedding_matrix(word_to_vec_map, \n",
    "                                word_to_index, \n",
    "                                emb_mean, emb_std):\n",
    "    \n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution\n",
    "    embed_matrix = np.random.normal(emb_mean, \n",
    "                                    emb_std, \n",
    "                                    (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th \n",
    "    # word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            \n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# Model Definitions\n",
    "\n",
    "def define_model(filters = 100, kernel_size = 3, \n",
    "                 activation='relu', input_dim = None, \n",
    "                 output_dim=300, max_length = None ):\n",
    "  \n",
    "    # Channel 1\n",
    "    input1 = Input(shape=(max_length,))\n",
    "    embeddding1 = Embedding(input_dim=input_dim, \n",
    "                            output_dim=output_dim, \n",
    "                            input_length=max_length)(input1)\n",
    "    conv1 = Conv1D(filters=filters, \n",
    "                   kernel_size=kernel_size, \n",
    "                   activation='relu', \n",
    "                   kernel_constraint= MaxNorm(max_value=3, \n",
    "                                              axis=[0,1])\n",
    "                  )(embeddding1)\n",
    "    pool1 = MaxPool1D(pool_size=2, strides=2)(conv1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    drop1 = Dropout(0.5)(flat1)\n",
    "    dense1 = Dense(10, activation='relu')(drop1)\n",
    "    drop1 = Dropout(0.5)(dense1)\n",
    "    out1 = Dense(1, activation='sigmoid')(drop1)\n",
    "    \n",
    "    # Channel 2\n",
    "    input2 = Input(shape=(max_length,))\n",
    "    embeddding2 = Embedding(input_dim=input_dim, \n",
    "                            output_dim=output_dim, \n",
    "                            input_length=max_length, \n",
    "                            mask_zero=True)(input2)\n",
    "    \n",
    "    gru2 = Bidirectional(GRU(64))(embeddding2)\n",
    "    drop2 = Dropout(0.5)(gru2)\n",
    "    out2 = Dense(1, activation='sigmoid')(drop2)\n",
    "    \n",
    "    # Merge\n",
    "    merged = concatenate([out1, out2])\n",
    "    \n",
    "    # Interpretation\n",
    "    outputs = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[input1, input2], outputs=outputs)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile( loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def define_model_2(filters = 100, kernel_size = 3, \n",
    "                   activation='relu', \n",
    "                   input_dim = None, output_dim=300, \n",
    "                   max_length = None, emb_matrix = None):\n",
    "  \n",
    "    # Channel 1\n",
    "    input1 = Input(shape=(max_length,))\n",
    "    embeddding1 = Embedding(input_dim=input_dim, \n",
    "                            output_dim=output_dim, \n",
    "                            input_length=max_length, \n",
    "                            input_shape=(max_length, ),\n",
    "                            # Assign the embedding weight \n",
    "                            # with word2vec embedding marix\n",
    "                            weights = [emb_matrix],\n",
    "                            # Set the weight to be not trainable \n",
    "                            # (static)\n",
    "                            trainable = False)(input1)\n",
    "    conv1 = Conv1D(filters=filters, \n",
    "                   kernel_size=kernel_size, \n",
    "                   activation='relu', \n",
    "                   kernel_constraint= MaxNorm(max_value=3, \n",
    "                                              axis=[0,1])\n",
    "                  )(embeddding1)\n",
    "    pool1 = MaxPool1D(pool_size=2, strides=2)(conv1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    drop1 = Dropout(0.5)(flat1)\n",
    "    dense1 = Dense(10, activation='relu')(drop1)\n",
    "    drop1 = Dropout(0.5)(dense1)\n",
    "    out1 = Dense(1, activation='sigmoid')(drop1)\n",
    "    \n",
    "    # Channel 2\n",
    "    input2 = Input(shape=(max_length,))\n",
    "    embeddding2 = Embedding(input_dim=input_dim, \n",
    "                            output_dim=output_dim, \n",
    "                            input_length=max_length, \n",
    "                            input_shape=(max_length, ),\n",
    "                            # Assign the embedding weight \n",
    "                            # with word2vec embedding marix\n",
    "                            weights = [emb_matrix],\n",
    "                            # Set the weight to be not trainable \n",
    "                            # (static)\n",
    "                            trainable = False,\n",
    "                            mask_zero=True)(input2)\n",
    "    \n",
    "    gru2 = Bidirectional(GRU(64))(embeddding2)\n",
    "    drop2 = Dropout(0.5)(gru2)\n",
    "    out2 = Dense(1, activation='sigmoid')(drop2)\n",
    "    \n",
    "    # Merge\n",
    "    merged = concatenate([out1, out2])\n",
    "    \n",
    "    # Interpretation\n",
    "    outputs = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[input1, input2], outputs=outputs)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile( loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def define_model_3(filters = 100, kernel_size = 3, \n",
    "                   activation='relu', \n",
    "                   input_dim = None, output_dim=300, \n",
    "                   max_length = None, emb_matrix = None):\n",
    "  \n",
    "    # Channel 1\n",
    "    input1 = Input(shape=(max_length,))\n",
    "    embeddding1 = Embedding(input_dim=input_dim, \n",
    "                            output_dim=output_dim, \n",
    "                            input_length=max_length, \n",
    "                            input_shape=(max_length, ),\n",
    "                            # Assign the embedding weight \n",
    "                            # with word2vec embedding marix\n",
    "                            weights = [emb_matrix],\n",
    "                            # Set the weight to be not trainable \n",
    "                            # (static)\n",
    "                            trainable = True)(input1)\n",
    "    conv1 = Conv1D(filters=filters, \n",
    "                   kernel_size=kernel_size, \n",
    "                   activation='relu', \n",
    "                   kernel_constraint= MaxNorm(max_value=3, \n",
    "                                              axis=[0,1])\n",
    "                  )(embeddding1)\n",
    "    pool1 = MaxPool1D(pool_size=2, strides=2)(conv1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    drop1 = Dropout(0.5)(flat1)\n",
    "    dense1 = Dense(10, activation='relu')(drop1)\n",
    "    drop1 = Dropout(0.5)(dense1)\n",
    "    out1 = Dense(1, activation='sigmoid')(drop1)\n",
    "    \n",
    "    # Channel 2\n",
    "    input2 = Input(shape=(max_length,))\n",
    "    embeddding2 = Embedding(input_dim=input_dim, \n",
    "                            output_dim=output_dim, \n",
    "                            input_length=max_length, \n",
    "                            input_shape=(max_length, ),\n",
    "                            # Assign the embedding weight \n",
    "                            # with word2vec embedding marix\n",
    "                            weights = [emb_matrix],\n",
    "                            # Set the weight to be not trainable \n",
    "                            # (static)\n",
    "                            trainable = True,\n",
    "                            mask_zero=True)(input2)\n",
    "    \n",
    "    gru2 = Bidirectional(GRU(64))(embeddding2)\n",
    "    drop2 = Dropout(0.5)(gru2)\n",
    "    out2 = Dense(1, activation='sigmoid')(drop2)\n",
    "    \n",
    "    # Merge\n",
    "    merged = concatenate([out1, out2])\n",
    "    \n",
    "    # Interpretation\n",
    "    outputs = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[input1, input2], outputs=outputs)\n",
    "    \n",
    "    # Compile\n",
    "    model.compile( loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the early stopping callbacks\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            min_delta=0, \n",
    "            patience=10, verbose=2, \n",
    "            mode='auto', \n",
    "            restore_best_weights=True)\n",
    "\n",
    "#==============================Step 3============================#\n",
    "# Begin the training process with random/static/dynamic Word2Vec\n",
    "\n",
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "activations = ['relu', 'tanh']\n",
    "filters = 100\n",
    "kernel_sizes = [1, 2, 3, 4, 5, 6]\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "\n",
    "columns = ['Activation', 'Filters', 'acc1', 'acc2', 'acc3', \n",
    "           'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', \n",
    "           'acc10', 'AVG']\n",
    "\n",
    "record_1 = pd.DataFrame(columns = columns)\n",
    "record_2 = pd.DataFrame(columns = columns)\n",
    "record_3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "for activation in activations:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        # kfold.split() will return set indices for each split\n",
    "        acc_list_1 = []\n",
    "        acc_list_2 = []\n",
    "        acc_list_3 = []\n",
    "        \n",
    "        for train, test in kfold.split(sentences):\n",
    "            \n",
    "            train_x, test_x = [], []\n",
    "            train_y, test_y = [], []\n",
    "            \n",
    "            for i in train:\n",
    "                train_x.append(sentences[i])\n",
    "                train_y.append(labels[i])\n",
    "\n",
    "            for i in test:\n",
    "                test_x.append(sentences[i])\n",
    "                test_y.append(labels[i])\n",
    "\n",
    "            # Turn the labels into a numpy array\n",
    "            train_y = np.array(train_y)\n",
    "            test_y = np.array(test_y)\n",
    "\n",
    "            # encode data using\n",
    "            # Cleaning and Tokenization\n",
    "            tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "            tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "            # Turn the text into sequence\n",
    "            training_sequences = tokenizer.texts_to_sequences(\n",
    "                                 train_x)\n",
    "            test_sequences = tokenizer.texts_to_sequences(\n",
    "                             test_x)\n",
    "\n",
    "            max_len = max_length(training_sequences)\n",
    "\n",
    "            # Pad the sequence to have the same size\n",
    "            Xtrain = pad_sequences(training_sequences, \n",
    "                                   maxlen=max_len, \n",
    "                                   padding=padding_type, \n",
    "                                   truncating=trunc_type)\n",
    "            Xtest = pad_sequences(test_sequences, \n",
    "                                  maxlen=max_len, \n",
    "                                  padding=padding_type, \n",
    "                                  truncating=trunc_type)\n",
    "\n",
    "            word_index = tokenizer.word_index\n",
    "            vocab_size = len(word_index)+1\n",
    "            \n",
    "            \n",
    "            emb_matrix = pretrained_embedding_matrix(word2vec, \n",
    "                                                     word_index, \n",
    "                                                     emb_mean, \n",
    "                                                     emb_std)\n",
    "            \n",
    "            # Define the models to train\n",
    "            model_1 = define_model(filters, \n",
    "                                   kernel_size, \n",
    "                                   activation, \n",
    "                                   input_dim=vocab_size, \n",
    "                                   max_length=max_len)\n",
    "            \n",
    "            model_2 = define_model_2(filters, \n",
    "                                     kernel_size, \n",
    "                                     activation, \n",
    "                                     input_dim=vocab_size, \n",
    "                                     max_length=max_len, \n",
    "                                     emb_matrix=emb_matrix)\n",
    "            \n",
    "            model_3 = define_model_3(filters, \n",
    "                                     kernel_size, \n",
    "                                     activation, \n",
    "                                     input_dim=vocab_size, \n",
    "                                     max_length=max_len, \n",
    "                                     emb_matrix=emb_matrix)\n",
    "            \n",
    "\n",
    "            # Train the models\n",
    "            model_1.fit(x=[Xtrain, Xtrain], train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1, \n",
    "                        callbacks=[callbacks], \n",
    "                        validation_data=([Xtest, Xtest], test_y))\n",
    "\n",
    "            model_2.fit(x=[Xtrain, Xtrain], train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1,\n",
    "                        callbacks=[callbacks], \n",
    "                        validation_data=([Xtest, Xtest], test_y))\n",
    "            \n",
    "            model_3.fit(x=[Xtrain, Xtrain], train_y, batch_size=50, \n",
    "                        epochs=100, verbose=1, \n",
    "                      callbacks=[callbacks], \n",
    "                        validation_data=([Xtest, Xtest], test_y))\n",
    "\n",
    "            # evaluate the model\n",
    "            loss_1, acc_1 = model_1.evaluate([Xtest, Xtest], test_y)\n",
    "            loss_2, acc_2 = model_2.evaluate([Xtest, Xtest], test_y)\n",
    "            loss_3, acc_3 = model_3.evaluate([Xtest, Xtest], test_y)\n",
    "\n",
    "            acc_list_1.append(acc*100)\n",
    "            acc_list_2.append(acc*100)\n",
    "            acc_list_3.append(acc*100)\n",
    "            \n",
    "        mean_acc_1 = np.array(acc_list_1).mean()\n",
    "        mean_acc_2 = np.array(acc_list_2).mean()\n",
    "        mean_acc_3 = np.array(acc_list_3).mean()\n",
    "        \n",
    "        parameters = [activation, kernel_size]\n",
    "        entries_1 = parameters + acc_list + [mean_acc_1]\n",
    "        entries_2 = parameters + acc_list + [mean_acc_2]\n",
    "        entries_3 = parameters + acc_list + [mean_acc_3]\n",
    "        \n",
    "        temp = pd.DataFrame([entries_1], columns=columns)\n",
    "        record_1 = record_1.append(temp, ignore_index=True)\n",
    "        temp = pd.DataFrame([entries_2], columns=columns)\n",
    "        record_2 = record_2.append(temp, ignore_index=True)\n",
    "        temp = pd.DataFrame([entries_3], columns=columns)\n",
    "        record_3 = record_3.append(temp, ignore_index=True)\n",
    "\n",
    "        print(record_1)\n",
    "        print(record_2)\n",
    "        print(record_3)\n",
    "\n",
    "#==============================Step 5============================#\n",
    "# Save the dataframe into excel file\n",
    "record_1.to_excel('WE_Ensemble_1.xlsx', sheet_name='random')\n",
    "record_2.to_excel('WE_Ensemble_2.xlsx', sheet_name='static')\n",
    "record_3.to_excel('WE_Ensemble_3.xlsx', sheet_name='dynamic')\n",
    "#================================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
