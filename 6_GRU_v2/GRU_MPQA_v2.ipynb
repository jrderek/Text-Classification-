{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtKoFBooZT-p"
   },
   "source": [
    "# GRU Classification with MPQA Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using GRU model on the MPQA Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnbrBF4UZT-1",
    "outputId": "da6c3538-99aa-4f22-ade2-eecbaaef5999"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BzHns_OcZT-3",
    "outputId": "2e46186c-3b5b-4d2c-81b6-3dcde438cf62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vYOheggZT-6"
   },
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "SXxjpeCrZT-6",
    "outputId": "be4cf304-2d68-48b9-ada2-3936499d1384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10606, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complaining</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>failing to support</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desperately needs</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>many years of decay</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no quick fix</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>urged</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10602</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10603</th>\n",
       "      <td>hope</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>strictly abide</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10605</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10606 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  sentence  label  split\n",
       "0              complaining      0  train\n",
       "1       failing to support      0  train\n",
       "2        desperately needs      0  train\n",
       "3      many years of decay      0  train\n",
       "4             no quick fix      0  train\n",
       "...                    ...    ...    ...\n",
       "10601                urged      1  train\n",
       "10602       strictly abide      1  train\n",
       "10603                 hope      1  train\n",
       "10604       strictly abide      1  train\n",
       "10605                           1  train\n",
       "\n",
       "[10606 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/MPQA/MPQA.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mM2puyxaZT-7",
    "outputId": "e01daff8-7206-4f19-ea8d-712141efd59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10606 entries, 0 to 10605\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10606 non-null  object\n",
      " 1   label     10606 non-null  int32 \n",
      " 2   split     10606 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 207.3+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "2e3Rn9MqZT-8",
    "outputId": "38a6fbaa-2a9c-4571-9978-23475eedab70"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7294</td>\n",
       "      <td>7294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3312</td>\n",
       "      <td>3312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          7294   7294\n",
       "1          3312   3312"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fdKkYCA1ZT-9"
   },
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "iSigZudxZT--",
    "outputId": "99a13387-79ac-49f1-ef03-0f8e5a289d27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'complaining'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hyCj1-SZT--"
   },
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfWIv-akZT-_"
   },
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pixqxgYDZT_A"
   },
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhRgyJJFZT_B",
    "outputId": "904eebdc-cbe7-4515-ce05-a2ea249bc73c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  no quick fix\n",
      "Into a sequence of int: [25, 945, 1476]\n",
      "Into a padded sequence: [  25  945 1476    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlJFsA50ZT_C",
    "outputId": "7e807ce7-5a88-40dc-d9a8-617f533f66ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "of 3\n",
      "to 4\n",
      "a 5\n",
      "and 6\n",
      "not 7\n",
      "is 8\n",
      "in 9\n",
      "be 10\n",
      "6236\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQJqTkckZT_D"
   },
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmAeP4v0ZT_D"
   },
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "g7EmzadeZT_E"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "        # tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iq60sF4ZT_F",
    "outputId": "f125e26f-1935-40b7-e870-4479b4ee56b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_20 (Bidirectio (None, 100, 128)          140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,169\n",
      "Trainable params: 515,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iLlATCocZT_G"
   },
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HXqi67SZT_G"
   },
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJXCNcV0ZT_H",
    "outputId": "bf091838-5bc1-4708-904f-e81d0f771ef3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 26s 48ms/step - loss: 0.5407 - accuracy: 0.7347 - val_loss: 0.3778 - val_accuracy: 0.8445\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1923 - accuracy: 0.9330 - val_loss: 0.3706 - val_accuracy: 0.8407\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1330 - accuracy: 0.9545 - val_loss: 0.4169 - val_accuracy: 0.8520\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1046 - accuracy: 0.9634 - val_loss: 0.4712 - val_accuracy: 0.8388\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.0761 - accuracy: 0.9697 - val_loss: 0.4566 - val_accuracy: 0.8539\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0585 - accuracy: 0.9761 - val_loss: 0.5242 - val_accuracy: 0.8520\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0466 - accuracy: 0.9824 - val_loss: 0.5278 - val_accuracy: 0.8596\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0474 - accuracy: 0.9807 - val_loss: 0.5889 - val_accuracy: 0.8520\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0404 - accuracy: 0.9838 - val_loss: 0.6188 - val_accuracy: 0.8473\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0344 - accuracy: 0.9838 - val_loss: 0.6686 - val_accuracy: 0.8473\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0343 - accuracy: 0.9850 - val_loss: 0.6929 - val_accuracy: 0.8464\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0357 - accuracy: 0.9836 - val_loss: 0.7315 - val_accuracy: 0.8360\n",
      "Epoch 13/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0304 - accuracy: 0.9855 - val_loss: 0.7951 - val_accuracy: 0.8379\n",
      "Epoch 14/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0253 - accuracy: 0.9887 - val_loss: 0.7431 - val_accuracy: 0.8464\n",
      "Epoch 15/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0318 - accuracy: 0.9841 - val_loss: 0.7923 - val_accuracy: 0.8435\n",
      "Epoch 16/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0291 - accuracy: 0.9870 - val_loss: 0.8175 - val_accuracy: 0.8426\n",
      "Epoch 17/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0276 - accuracy: 0.9862 - val_loss: 0.8721 - val_accuracy: 0.8426\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 85.95664501190186\n",
      "Training 2: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 27s 46ms/step - loss: 0.5368 - accuracy: 0.7388 - val_loss: 0.3326 - val_accuracy: 0.8709\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 10s 34ms/step - loss: 0.1865 - accuracy: 0.9348 - val_loss: 0.3522 - val_accuracy: 0.8615\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 10s 34ms/step - loss: 0.1308 - accuracy: 0.9564 - val_loss: 0.3663 - val_accuracy: 0.8718\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1021 - accuracy: 0.9638 - val_loss: 0.4240 - val_accuracy: 0.8596\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0765 - accuracy: 0.9726 - val_loss: 0.4615 - val_accuracy: 0.8530\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0554 - accuracy: 0.9782 - val_loss: 0.5228 - val_accuracy: 0.8549\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0484 - accuracy: 0.9819 - val_loss: 0.5289 - val_accuracy: 0.8624\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0410 - accuracy: 0.9845 - val_loss: 0.5710 - val_accuracy: 0.8615\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0409 - accuracy: 0.9822 - val_loss: 0.6302 - val_accuracy: 0.8643\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0349 - accuracy: 0.9852 - val_loss: 0.6506 - val_accuracy: 0.8680\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0374 - accuracy: 0.9820 - val_loss: 0.6490 - val_accuracy: 0.8643\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0331 - accuracy: 0.9855 - val_loss: 0.7359 - val_accuracy: 0.8605\n",
      "Epoch 13/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0288 - accuracy: 0.9870 - val_loss: 0.7258 - val_accuracy: 0.8586\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 87.1819019317627\n",
      "Training 3: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 28s 49ms/step - loss: 0.5314 - accuracy: 0.7380 - val_loss: 0.4104 - val_accuracy: 0.8473\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.2005 - accuracy: 0.9291 - val_loss: 0.4616 - val_accuracy: 0.8369\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1179 - accuracy: 0.9576 - val_loss: 0.4640 - val_accuracy: 0.8445\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0909 - accuracy: 0.9683 - val_loss: 0.5005 - val_accuracy: 0.8351\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0699 - accuracy: 0.9743 - val_loss: 0.5672 - val_accuracy: 0.8351\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0550 - accuracy: 0.9793 - val_loss: 0.6132 - val_accuracy: 0.8388\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0438 - accuracy: 0.9827 - val_loss: 0.6787 - val_accuracy: 0.8398\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0398 - accuracy: 0.9838 - val_loss: 0.7633 - val_accuracy: 0.8313\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0354 - accuracy: 0.9857 - val_loss: 0.7768 - val_accuracy: 0.8351\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0292 - accuracy: 0.9860 - val_loss: 0.8331 - val_accuracy: 0.8143\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0327 - accuracy: 0.9848 - val_loss: 0.7886 - val_accuracy: 0.8388\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 84.73138809204102\n",
      "Training 4: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 28s 49ms/step - loss: 0.5318 - accuracy: 0.7494 - val_loss: 0.3450 - val_accuracy: 0.8558\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.2005 - accuracy: 0.9289 - val_loss: 0.3472 - val_accuracy: 0.8275\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1347 - accuracy: 0.9569 - val_loss: 0.3642 - val_accuracy: 0.8615\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0977 - accuracy: 0.9665 - val_loss: 0.4238 - val_accuracy: 0.8303\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0714 - accuracy: 0.9723 - val_loss: 0.4911 - val_accuracy: 0.8633\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0508 - accuracy: 0.9814 - val_loss: 0.5163 - val_accuracy: 0.8511\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0473 - accuracy: 0.9813 - val_loss: 0.5846 - val_accuracy: 0.8596\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0457 - accuracy: 0.9830 - val_loss: 0.6190 - val_accuracy: 0.8190\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0387 - accuracy: 0.9825 - val_loss: 0.6603 - val_accuracy: 0.8624\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0337 - accuracy: 0.9856 - val_loss: 0.6514 - val_accuracy: 0.8539\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0335 - accuracy: 0.9857 - val_loss: 0.7127 - val_accuracy: 0.8567\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0264 - accuracy: 0.9879 - val_loss: 0.7353 - val_accuracy: 0.8501\n",
      "Epoch 13/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.0281 - accuracy: 0.9856 - val_loss: 0.7793 - val_accuracy: 0.8294\n",
      "Epoch 14/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0260 - accuracy: 0.9878 - val_loss: 0.8328 - val_accuracy: 0.8492\n",
      "Epoch 15/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.0293 - accuracy: 0.9872 - val_loss: 0.7749 - val_accuracy: 0.8501\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 86.33365035057068\n",
      "Training 5: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 27s 48ms/step - loss: 0.5390 - accuracy: 0.7470 - val_loss: 0.3601 - val_accuracy: 0.8539\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1999 - accuracy: 0.9283 - val_loss: 0.3883 - val_accuracy: 0.8520\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.1433 - accuracy: 0.9471 - val_loss: 0.4110 - val_accuracy: 0.8586\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.0982 - accuracy: 0.9638 - val_loss: 0.4449 - val_accuracy: 0.8586\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0742 - accuracy: 0.9725 - val_loss: 0.5316 - val_accuracy: 0.8388\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0592 - accuracy: 0.9759 - val_loss: 0.5375 - val_accuracy: 0.8483\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0453 - accuracy: 0.9800 - val_loss: 0.6087 - val_accuracy: 0.8501\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0420 - accuracy: 0.9814 - val_loss: 0.5706 - val_accuracy: 0.8341\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 10s 34ms/step - loss: 0.0343 - accuracy: 0.9856 - val_loss: 0.6429 - val_accuracy: 0.8483\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0330 - accuracy: 0.9855 - val_loss: 0.6476 - val_accuracy: 0.8388\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0330 - accuracy: 0.9849 - val_loss: 0.7474 - val_accuracy: 0.8388\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0352 - accuracy: 0.9814 - val_loss: 0.7626 - val_accuracy: 0.8379\n",
      "Epoch 13/30\n",
      "299/299 [==============================] - 10s 35ms/step - loss: 0.0351 - accuracy: 0.9847 - val_loss: 0.8138 - val_accuracy: 0.8294\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 85.86239218711853\n",
      "Training 6: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 27s 49ms/step - loss: 0.5340 - accuracy: 0.7557 - val_loss: 0.3426 - val_accuracy: 0.8690\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.1997 - accuracy: 0.9306 - val_loss: 0.3529 - val_accuracy: 0.8652\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1340 - accuracy: 0.9520 - val_loss: 0.4017 - val_accuracy: 0.8530\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0990 - accuracy: 0.9646 - val_loss: 0.4638 - val_accuracy: 0.8483\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0756 - accuracy: 0.9715 - val_loss: 0.5119 - val_accuracy: 0.8586\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0542 - accuracy: 0.9784 - val_loss: 0.5450 - val_accuracy: 0.8492\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.0500 - accuracy: 0.9802 - val_loss: 0.6243 - val_accuracy: 0.8511\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0375 - accuracy: 0.9850 - val_loss: 0.5934 - val_accuracy: 0.8520\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.0411 - accuracy: 0.9822 - val_loss: 0.6083 - val_accuracy: 0.8549\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.0345 - accuracy: 0.9848 - val_loss: 0.6657 - val_accuracy: 0.8454\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.0351 - accuracy: 0.9836 - val_loss: 0.7385 - val_accuracy: 0.8473\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 86.8991494178772\n",
      "Training 7: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 27s 49ms/step - loss: 0.5376 - accuracy: 0.7431 - val_loss: 0.3525 - val_accuracy: 0.8500\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1988 - accuracy: 0.9271 - val_loss: 0.3890 - val_accuracy: 0.8472\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.1384 - accuracy: 0.9517 - val_loss: 0.4128 - val_accuracy: 0.8528\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0974 - accuracy: 0.9646 - val_loss: 0.4800 - val_accuracy: 0.8434\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.0735 - accuracy: 0.9737 - val_loss: 0.5307 - val_accuracy: 0.8302\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.0585 - accuracy: 0.9781 - val_loss: 0.6168 - val_accuracy: 0.8434\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0474 - accuracy: 0.9815 - val_loss: 0.6452 - val_accuracy: 0.8358\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 11s 35ms/step - loss: 0.0452 - accuracy: 0.9798 - val_loss: 0.6260 - val_accuracy: 0.8443\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0382 - accuracy: 0.9818 - val_loss: 0.7129 - val_accuracy: 0.8425\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0391 - accuracy: 0.9832 - val_loss: 0.7156 - val_accuracy: 0.8396\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0313 - accuracy: 0.9870 - val_loss: 0.7918 - val_accuracy: 0.8358\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0310 - accuracy: 0.9844 - val_loss: 0.8778 - val_accuracy: 0.8311\n",
      "Epoch 13/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0359 - accuracy: 0.9823 - val_loss: 0.7899 - val_accuracy: 0.8387\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 85.2830171585083\n",
      "Training 8: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 27s 49ms/step - loss: 0.5418 - accuracy: 0.7416 - val_loss: 0.3875 - val_accuracy: 0.8585\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.2001 - accuracy: 0.9303 - val_loss: 0.3899 - val_accuracy: 0.8632\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1372 - accuracy: 0.9560 - val_loss: 0.4374 - val_accuracy: 0.8585\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.0956 - accuracy: 0.9657 - val_loss: 0.4663 - val_accuracy: 0.8670\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0773 - accuracy: 0.9713 - val_loss: 0.5090 - val_accuracy: 0.8594\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0550 - accuracy: 0.9779 - val_loss: 0.5517 - val_accuracy: 0.8594\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0460 - accuracy: 0.9821 - val_loss: 0.5909 - val_accuracy: 0.8585\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0423 - accuracy: 0.9810 - val_loss: 0.6127 - val_accuracy: 0.8500\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0395 - accuracy: 0.9819 - val_loss: 0.7508 - val_accuracy: 0.8340\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0332 - accuracy: 0.9853 - val_loss: 0.7210 - val_accuracy: 0.8519\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0325 - accuracy: 0.9838 - val_loss: 0.8267 - val_accuracy: 0.8453\n",
      "Epoch 12/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0348 - accuracy: 0.9826 - val_loss: 0.7969 - val_accuracy: 0.8500\n",
      "Epoch 13/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0269 - accuracy: 0.9876 - val_loss: 0.9158 - val_accuracy: 0.8321\n",
      "Epoch 14/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0333 - accuracy: 0.9833 - val_loss: 0.8092 - val_accuracy: 0.8443\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 86.69811487197876\n",
      "Training 9: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 27s 49ms/step - loss: 0.5343 - accuracy: 0.7539 - val_loss: 0.3626 - val_accuracy: 0.8462\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1920 - accuracy: 0.9324 - val_loss: 0.4218 - val_accuracy: 0.8377\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1250 - accuracy: 0.9554 - val_loss: 0.4255 - val_accuracy: 0.8377\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0999 - accuracy: 0.9655 - val_loss: 0.4627 - val_accuracy: 0.8349\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0751 - accuracy: 0.9728 - val_loss: 0.5369 - val_accuracy: 0.8396\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0576 - accuracy: 0.9755 - val_loss: 0.5991 - val_accuracy: 0.8377\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0435 - accuracy: 0.9843 - val_loss: 0.6215 - val_accuracy: 0.8377\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0430 - accuracy: 0.9828 - val_loss: 0.6682 - val_accuracy: 0.8434\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0376 - accuracy: 0.9837 - val_loss: 0.6965 - val_accuracy: 0.8434\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0348 - accuracy: 0.9833 - val_loss: 0.7302 - val_accuracy: 0.8358\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0373 - accuracy: 0.9825 - val_loss: 0.7271 - val_accuracy: 0.8368\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 84.62263941764832\n",
      "Training 10: \n",
      "Epoch 1/30\n",
      "299/299 [==============================] - 27s 49ms/step - loss: 0.5290 - accuracy: 0.7470 - val_loss: 0.3591 - val_accuracy: 0.8509\n",
      "Epoch 2/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.1960 - accuracy: 0.9363 - val_loss: 0.3871 - val_accuracy: 0.8509\n",
      "Epoch 3/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.1224 - accuracy: 0.9563 - val_loss: 0.4424 - val_accuracy: 0.8377\n",
      "Epoch 4/30\n",
      "299/299 [==============================] - 11s 38ms/step - loss: 0.1006 - accuracy: 0.9656 - val_loss: 0.5181 - val_accuracy: 0.8415\n",
      "Epoch 5/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0747 - accuracy: 0.9701 - val_loss: 0.5729 - val_accuracy: 0.8340\n",
      "Epoch 6/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0578 - accuracy: 0.9778 - val_loss: 0.6354 - val_accuracy: 0.8302\n",
      "Epoch 7/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0433 - accuracy: 0.9832 - val_loss: 0.6575 - val_accuracy: 0.8340\n",
      "Epoch 8/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0341 - accuracy: 0.9859 - val_loss: 0.7295 - val_accuracy: 0.8217\n",
      "Epoch 9/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0344 - accuracy: 0.9866 - val_loss: 0.7071 - val_accuracy: 0.8274\n",
      "Epoch 10/30\n",
      "299/299 [==============================] - 11s 37ms/step - loss: 0.0320 - accuracy: 0.9865 - val_loss: 0.7823 - val_accuracy: 0.8198\n",
      "Epoch 11/30\n",
      "299/299 [==============================] - 11s 36ms/step - loss: 0.0329 - accuracy: 0.9860 - val_loss: 0.8406 - val_accuracy: 0.8340\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 85.0943386554718\n",
      "\n",
      "        acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
      "0  85.956645  87.181902  84.731388  ...  84.622639  85.094339  85.866324\n",
      "\n",
      "[1 rows x 11 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=30, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cfa0i7wiZT_I"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "levj5is5ZT_J",
    "outputId": "155b98c5-ecdc-4430-e964-d069b5736aa2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.956645</td>\n",
       "      <td>87.181902</td>\n",
       "      <td>84.731388</td>\n",
       "      <td>86.33365</td>\n",
       "      <td>85.862392</td>\n",
       "      <td>86.899149</td>\n",
       "      <td>85.283017</td>\n",
       "      <td>86.698115</td>\n",
       "      <td>84.622639</td>\n",
       "      <td>85.094339</td>\n",
       "      <td>85.866324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3  ...       acc9      acc10        AVG\n",
       "0  85.956645  87.181902  84.731388  ...  84.622639  85.094339  85.866324\n",
       "\n",
       "[1 rows x 11 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1vOEUHxuZT_K"
   },
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('GRU_MPQA_v2.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCHsRGvtZT_K"
   },
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYuP2HtUZT_L"
   },
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flHiS9HKZT_L"
   },
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "J3XdEOfHZT_M"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1QcTxqRZT_M",
    "outputId": "c51329f2-4aee-458f-c00b-26db3774a21e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTNqKy9QZT_N"
   },
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4dUNC7vHZT_N"
   },
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVLl6t6pZT_O",
    "outputId": "fde2e3fe-80e1-4a4f-e1ce-b4ab1087fc37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6083 words present from 6236 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA3okYHyZT_O"
   },
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_mean:  -0.003527845\n",
      "emb_std:  0.13315111\n"
     ]
    }
   ],
   "source": [
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "print('emb_mean: ', emb_mean)\n",
    "print('emb_std: ', emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "M1wLZvxmZT_P"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution values\n",
    "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UnHVdZuPZT_P",
    "outputId": "9fceb681-b4b9-4aef-9fce-cd20005be854"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
       "        -0.09975549, -0.08595189],\n",
       "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
       "        -0.13156594, -0.05996158],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6mQTpiQZT_Q"
   },
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "FnF5yyKHZT_Q"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "        # tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KYzRbWEMZT_T",
    "outputId": "16b012f3-a113-4788-9b62-0798399a7f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 128)          140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,169\n",
      "Trainable params: 215,169\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asRYvjzyZT_T"
   },
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lEhNTDbDZT_T"
   },
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=10, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3ZvNNvIZT_U",
    "outputId": "270c9916-cdf3-4df0-d3ac-fef71a9e97e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 45s 94ms/step - loss: 0.4234 - accuracy: 0.8070 - val_loss: 0.2990 - val_accuracy: 0.8784\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 20s 68ms/step - loss: 0.2867 - accuracy: 0.8887 - val_loss: 0.2983 - val_accuracy: 0.8794\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 20s 68ms/step - loss: 0.2485 - accuracy: 0.9063 - val_loss: 0.3052 - val_accuracy: 0.8728\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.2158 - accuracy: 0.9165 - val_loss: 0.3022 - val_accuracy: 0.8831\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 20s 69ms/step - loss: 0.2029 - accuracy: 0.9195 - val_loss: 0.3212 - val_accuracy: 0.8737\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 20s 68ms/step - loss: 0.1763 - accuracy: 0.9311 - val_loss: 0.3412 - val_accuracy: 0.8586\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.1464 - accuracy: 0.9460 - val_loss: 0.3483 - val_accuracy: 0.8615\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.1333 - accuracy: 0.9496 - val_loss: 0.4198 - val_accuracy: 0.8549\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 20s 67ms/step - loss: 0.1019 - accuracy: 0.9622 - val_loss: 0.4108 - val_accuracy: 0.8652\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.1063 - accuracy: 0.9587 - val_loss: 0.4181 - val_accuracy: 0.8690\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.0817 - accuracy: 0.9708 - val_loss: 0.5120 - val_accuracy: 0.8671\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 20s 66ms/step - loss: 0.0795 - accuracy: 0.9684 - val_loss: 0.4897 - val_accuracy: 0.8709\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 20s 68ms/step - loss: 0.0708 - accuracy: 0.9745 - val_loss: 0.5055 - val_accuracy: 0.8690\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 20s 68ms/step - loss: 0.0630 - accuracy: 0.9785 - val_loss: 0.5870 - val_accuracy: 0.8662\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 88.31291198730469\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 54s 119ms/step - loss: 0.4172 - accuracy: 0.8017 - val_loss: 0.3186 - val_accuracy: 0.8718\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 30s 101ms/step - loss: 0.2804 - accuracy: 0.8917 - val_loss: 0.3078 - val_accuracy: 0.8869\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.2534 - accuracy: 0.9021 - val_loss: 0.3004 - val_accuracy: 0.8831\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.2290 - accuracy: 0.9108 - val_loss: 0.3072 - val_accuracy: 0.8746\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.1989 - accuracy: 0.9194 - val_loss: 0.3027 - val_accuracy: 0.8860\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 31s 103ms/step - loss: 0.1813 - accuracy: 0.9298 - val_loss: 0.3168 - val_accuracy: 0.8860\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 32s 105ms/step - loss: 0.1471 - accuracy: 0.9435 - val_loss: 0.3335 - val_accuracy: 0.8812\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 32s 106ms/step - loss: 0.1383 - accuracy: 0.9489 - val_loss: 0.3941 - val_accuracy: 0.8841\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.1048 - accuracy: 0.9602 - val_loss: 0.4290 - val_accuracy: 0.8765\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 32s 108ms/step - loss: 0.0986 - accuracy: 0.9594 - val_loss: 0.4168 - val_accuracy: 0.8784\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 31s 105ms/step - loss: 0.0883 - accuracy: 0.9659 - val_loss: 0.4799 - val_accuracy: 0.8690\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 30s 99ms/step - loss: 0.0809 - accuracy: 0.9713 - val_loss: 0.5131 - val_accuracy: 0.8690\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 88.68991732597351\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 44s 88ms/step - loss: 0.4291 - accuracy: 0.8016 - val_loss: 0.2948 - val_accuracy: 0.8841\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 21s 70ms/step - loss: 0.2643 - accuracy: 0.8983 - val_loss: 0.2789 - val_accuracy: 0.8907\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.2457 - accuracy: 0.9042 - val_loss: 0.2768 - val_accuracy: 0.8944\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.2253 - accuracy: 0.9161 - val_loss: 0.2891 - val_accuracy: 0.8765\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.1983 - accuracy: 0.9224 - val_loss: 0.2795 - val_accuracy: 0.8935\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 21s 72ms/step - loss: 0.1698 - accuracy: 0.9319 - val_loss: 0.2941 - val_accuracy: 0.8935\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.1488 - accuracy: 0.9446 - val_loss: 0.3165 - val_accuracy: 0.8869\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.1354 - accuracy: 0.9466 - val_loss: 0.3480 - val_accuracy: 0.8869\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.1201 - accuracy: 0.9562 - val_loss: 0.3750 - val_accuracy: 0.8878\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.1042 - accuracy: 0.9580 - val_loss: 0.3786 - val_accuracy: 0.8916\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 22s 72ms/step - loss: 0.0918 - accuracy: 0.9634 - val_loss: 0.4269 - val_accuracy: 0.8794\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 21s 71ms/step - loss: 0.0754 - accuracy: 0.9732 - val_loss: 0.4892 - val_accuracy: 0.8812\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 21s 70ms/step - loss: 0.0656 - accuracy: 0.9767 - val_loss: 0.5269 - val_accuracy: 0.8869\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 89.44392204284668\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 72s 175ms/step - loss: 0.4140 - accuracy: 0.8093 - val_loss: 0.3188 - val_accuracy: 0.8728\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 50s 167ms/step - loss: 0.2612 - accuracy: 0.8959 - val_loss: 0.3075 - val_accuracy: 0.8709\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 47s 158ms/step - loss: 0.2453 - accuracy: 0.9060 - val_loss: 0.3130 - val_accuracy: 0.8746\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 45s 151ms/step - loss: 0.2214 - accuracy: 0.9145 - val_loss: 0.3046 - val_accuracy: 0.8709\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.2004 - accuracy: 0.9218 - val_loss: 0.3586 - val_accuracy: 0.8539\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 44s 149ms/step - loss: 0.1830 - accuracy: 0.9308 - val_loss: 0.3311 - val_accuracy: 0.8671\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 45s 150ms/step - loss: 0.1517 - accuracy: 0.9402 - val_loss: 0.3644 - val_accuracy: 0.8690\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 43s 145ms/step - loss: 0.1315 - accuracy: 0.9502 - val_loss: 0.3948 - val_accuracy: 0.8775\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 42s 141ms/step - loss: 0.1145 - accuracy: 0.9554 - val_loss: 0.4342 - val_accuracy: 0.8756\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 43s 143ms/step - loss: 0.1007 - accuracy: 0.9603 - val_loss: 0.4773 - val_accuracy: 0.8662\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 41s 138ms/step - loss: 0.0904 - accuracy: 0.9656 - val_loss: 0.5117 - val_accuracy: 0.8662\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 41s 138ms/step - loss: 0.0841 - accuracy: 0.9671 - val_loss: 0.5566 - val_accuracy: 0.8728\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 41s 138ms/step - loss: 0.0693 - accuracy: 0.9722 - val_loss: 0.6035 - val_accuracy: 0.8728\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 41s 139ms/step - loss: 0.0627 - accuracy: 0.9767 - val_loss: 0.6317 - val_accuracy: 0.8680\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 40s 135ms/step - loss: 0.0600 - accuracy: 0.9761 - val_loss: 0.7150 - val_accuracy: 0.8596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40\n",
      "299/299 [==============================] - 40s 134ms/step - loss: 0.0550 - accuracy: 0.9793 - val_loss: 0.7080 - val_accuracy: 0.8728\n",
      "Epoch 17/40\n",
      "299/299 [==============================] - 39s 131ms/step - loss: 0.0558 - accuracy: 0.9811 - val_loss: 0.7061 - val_accuracy: 0.8662\n",
      "Epoch 18/40\n",
      "299/299 [==============================] - 38s 127ms/step - loss: 0.0527 - accuracy: 0.9795 - val_loss: 0.6891 - val_accuracy: 0.8671\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00018: early stopping\n",
      "Test Accuracy: 87.74740695953369\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 53s 120ms/step - loss: 0.4205 - accuracy: 0.8094 - val_loss: 0.3394 - val_accuracy: 0.8586\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.2688 - accuracy: 0.8936 - val_loss: 0.3349 - val_accuracy: 0.8652\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 25s 85ms/step - loss: 0.2411 - accuracy: 0.9060 - val_loss: 0.3425 - val_accuracy: 0.8633\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 25s 83ms/step - loss: 0.2271 - accuracy: 0.9141 - val_loss: 0.3070 - val_accuracy: 0.8699\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 24s 81ms/step - loss: 0.2033 - accuracy: 0.9219 - val_loss: 0.3144 - val_accuracy: 0.8709\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 25s 84ms/step - loss: 0.1694 - accuracy: 0.9363 - val_loss: 0.3402 - val_accuracy: 0.8662\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.1609 - accuracy: 0.9351 - val_loss: 0.3447 - val_accuracy: 0.8765\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 24s 81ms/step - loss: 0.1269 - accuracy: 0.9509 - val_loss: 0.3585 - val_accuracy: 0.8728\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 24s 82ms/step - loss: 0.1147 - accuracy: 0.9562 - val_loss: 0.3989 - val_accuracy: 0.8662\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.0923 - accuracy: 0.9672 - val_loss: 0.4555 - val_accuracy: 0.8718\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.0814 - accuracy: 0.9685 - val_loss: 0.4842 - val_accuracy: 0.8643\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 24s 81ms/step - loss: 0.0702 - accuracy: 0.9719 - val_loss: 0.4937 - val_accuracy: 0.8671\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 23s 77ms/step - loss: 0.0609 - accuracy: 0.9763 - val_loss: 0.5538 - val_accuracy: 0.8699\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 24s 79ms/step - loss: 0.0594 - accuracy: 0.9761 - val_loss: 0.6478 - val_accuracy: 0.8624\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 24s 80ms/step - loss: 0.0524 - accuracy: 0.9800 - val_loss: 0.6466 - val_accuracy: 0.8699\n",
      "Epoch 16/40\n",
      "299/299 [==============================] - 24s 79ms/step - loss: 0.0515 - accuracy: 0.9800 - val_loss: 0.6449 - val_accuracy: 0.8690\n",
      "Epoch 17/40\n",
      "299/299 [==============================] - 24s 79ms/step - loss: 0.0474 - accuracy: 0.9810 - val_loss: 0.6576 - val_accuracy: 0.8671\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00017: early stopping\n",
      "Test Accuracy: 87.65316009521484\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 45s 97ms/step - loss: 0.4242 - accuracy: 0.8047 - val_loss: 0.2962 - val_accuracy: 0.8784\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 23s 76ms/step - loss: 0.2734 - accuracy: 0.8912 - val_loss: 0.2750 - val_accuracy: 0.8935\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 23s 76ms/step - loss: 0.2405 - accuracy: 0.9067 - val_loss: 0.2750 - val_accuracy: 0.8897\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 23s 76ms/step - loss: 0.2253 - accuracy: 0.9160 - val_loss: 0.2758 - val_accuracy: 0.8831\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 27s 91ms/step - loss: 0.1894 - accuracy: 0.9264 - val_loss: 0.2732 - val_accuracy: 0.8850\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 66s 222ms/step - loss: 0.1713 - accuracy: 0.9350 - val_loss: 0.2922 - val_accuracy: 0.8775\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 64s 213ms/step - loss: 0.1566 - accuracy: 0.9399 - val_loss: 0.3224 - val_accuracy: 0.8756\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 67s 225ms/step - loss: 0.1270 - accuracy: 0.9547 - val_loss: 0.3720 - val_accuracy: 0.8794\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 69s 230ms/step - loss: 0.1131 - accuracy: 0.9567 - val_loss: 0.4066 - val_accuracy: 0.8784\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 66s 222ms/step - loss: 0.0976 - accuracy: 0.9647 - val_loss: 0.4420 - val_accuracy: 0.8718\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 64s 215ms/step - loss: 0.0842 - accuracy: 0.9699 - val_loss: 0.4923 - val_accuracy: 0.8615\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 67s 226ms/step - loss: 0.0781 - accuracy: 0.9703 - val_loss: 0.4983 - val_accuracy: 0.8615\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 89.34966921806335\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 121s 347ms/step - loss: 0.4237 - accuracy: 0.7982 - val_loss: 0.3243 - val_accuracy: 0.8726\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 81s 270ms/step - loss: 0.2711 - accuracy: 0.8983 - val_loss: 0.2919 - val_accuracy: 0.8934\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 77s 258ms/step - loss: 0.2446 - accuracy: 0.9041 - val_loss: 0.3011 - val_accuracy: 0.8896\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 82s 275ms/step - loss: 0.2127 - accuracy: 0.9169 - val_loss: 0.2998 - val_accuracy: 0.8915\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 77s 257ms/step - loss: 0.2012 - accuracy: 0.9208 - val_loss: 0.3121 - val_accuracy: 0.8868\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 76s 256ms/step - loss: 0.1708 - accuracy: 0.9300 - val_loss: 0.3190 - val_accuracy: 0.8896\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 81s 270ms/step - loss: 0.1505 - accuracy: 0.9389 - val_loss: 0.3588 - val_accuracy: 0.8811\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 77s 259ms/step - loss: 0.1276 - accuracy: 0.9492 - val_loss: 0.4001 - val_accuracy: 0.8877\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 72s 242ms/step - loss: 0.1077 - accuracy: 0.9602 - val_loss: 0.4227 - val_accuracy: 0.8689\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 75s 249ms/step - loss: 0.0954 - accuracy: 0.9659 - val_loss: 0.4386 - val_accuracy: 0.8840\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 75s 250ms/step - loss: 0.0869 - accuracy: 0.9649 - val_loss: 0.4768 - val_accuracy: 0.8792\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 0.0887 - accuracy: 0.9659 - val_loss: 0.4851 - val_accuracy: 0.8887\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 89.33961987495422\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 77s 193ms/step - loss: 0.4088 - accuracy: 0.8151 - val_loss: 0.3293 - val_accuracy: 0.8632\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 50s 167ms/step - loss: 0.2742 - accuracy: 0.8908 - val_loss: 0.3340 - val_accuracy: 0.8660\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 51s 171ms/step - loss: 0.2458 - accuracy: 0.9059 - val_loss: 0.3116 - val_accuracy: 0.8708\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 51s 171ms/step - loss: 0.2111 - accuracy: 0.9190 - val_loss: 0.3184 - val_accuracy: 0.8689\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 51s 171ms/step - loss: 0.1980 - accuracy: 0.9200 - val_loss: 0.3220 - val_accuracy: 0.8717\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 52s 173ms/step - loss: 0.1757 - accuracy: 0.9313 - val_loss: 0.3433 - val_accuracy: 0.8632\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 51s 171ms/step - loss: 0.1464 - accuracy: 0.9465 - val_loss: 0.3726 - val_accuracy: 0.8670\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 51s 171ms/step - loss: 0.1271 - accuracy: 0.9504 - val_loss: 0.4055 - val_accuracy: 0.8670\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 51s 171ms/step - loss: 0.1145 - accuracy: 0.9583 - val_loss: 0.4199 - val_accuracy: 0.8594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40\n",
      "299/299 [==============================] - 50s 167ms/step - loss: 0.0963 - accuracy: 0.9636 - val_loss: 0.4624 - val_accuracy: 0.8632\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 49s 164ms/step - loss: 0.0821 - accuracy: 0.9709 - val_loss: 0.4649 - val_accuracy: 0.8679\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 49s 163ms/step - loss: 0.0652 - accuracy: 0.9761 - val_loss: 0.5437 - val_accuracy: 0.8585\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 49s 163ms/step - loss: 0.0652 - accuracy: 0.9752 - val_loss: 0.5398 - val_accuracy: 0.8717\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 49s 163ms/step - loss: 0.0617 - accuracy: 0.9769 - val_loss: 0.5268 - val_accuracy: 0.8717\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 49s 163ms/step - loss: 0.0675 - accuracy: 0.9763 - val_loss: 0.5674 - val_accuracy: 0.8708\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 87.16981410980225\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 123s 329ms/step - loss: 0.4210 - accuracy: 0.8083 - val_loss: 0.3596 - val_accuracy: 0.8566\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 77s 257ms/step - loss: 0.2680 - accuracy: 0.8985 - val_loss: 0.3327 - val_accuracy: 0.8670\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 0.2370 - accuracy: 0.9083 - val_loss: 0.3308 - val_accuracy: 0.8594\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 73s 246ms/step - loss: 0.2259 - accuracy: 0.9149 - val_loss: 0.3272 - val_accuracy: 0.8660\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 74s 247ms/step - loss: 0.2033 - accuracy: 0.9204 - val_loss: 0.3412 - val_accuracy: 0.8642\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 74s 248ms/step - loss: 0.1721 - accuracy: 0.9306 - val_loss: 0.3757 - val_accuracy: 0.8660\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 75s 251ms/step - loss: 0.1522 - accuracy: 0.9417 - val_loss: 0.3692 - val_accuracy: 0.8575\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 75s 251ms/step - loss: 0.1214 - accuracy: 0.9577 - val_loss: 0.4063 - val_accuracy: 0.8547\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 73s 246ms/step - loss: 0.1171 - accuracy: 0.9528 - val_loss: 0.4359 - val_accuracy: 0.8557\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0966 - accuracy: 0.9617 - val_loss: 0.4498 - val_accuracy: 0.8623\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 73s 244ms/step - loss: 0.0916 - accuracy: 0.9649 - val_loss: 0.5127 - val_accuracy: 0.8566\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 73s 243ms/step - loss: 0.0811 - accuracy: 0.9670 - val_loss: 0.5940 - val_accuracy: 0.8547\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 86.69811487197876\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 77s 194ms/step - loss: 0.3959 - accuracy: 0.8234 - val_loss: 0.3732 - val_accuracy: 0.8594\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 51s 170ms/step - loss: 0.2707 - accuracy: 0.8983 - val_loss: 0.3579 - val_accuracy: 0.8500\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 55s 184ms/step - loss: 0.2334 - accuracy: 0.9104 - val_loss: 0.3497 - val_accuracy: 0.8575\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 55s 183ms/step - loss: 0.2134 - accuracy: 0.9139 - val_loss: 0.3686 - val_accuracy: 0.8632\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 54s 182ms/step - loss: 0.1901 - accuracy: 0.9275 - val_loss: 0.3842 - val_accuracy: 0.8425\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 55s 184ms/step - loss: 0.1563 - accuracy: 0.9416 - val_loss: 0.3902 - val_accuracy: 0.8519\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 55s 184ms/step - loss: 0.1542 - accuracy: 0.9392 - val_loss: 0.4315 - val_accuracy: 0.8462\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 55s 184ms/step - loss: 0.1227 - accuracy: 0.9532 - val_loss: 0.4994 - val_accuracy: 0.8538\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 55s 184ms/step - loss: 0.1001 - accuracy: 0.9600 - val_loss: 0.5769 - val_accuracy: 0.8406\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 55s 182ms/step - loss: 0.0988 - accuracy: 0.9627 - val_loss: 0.6103 - val_accuracy: 0.8377\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 55s 182ms/step - loss: 0.0803 - accuracy: 0.9690 - val_loss: 0.6775 - val_accuracy: 0.8519\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 54s 181ms/step - loss: 0.0728 - accuracy: 0.9735 - val_loss: 0.7197 - val_accuracy: 0.8415\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 54s 181ms/step - loss: 0.0548 - accuracy: 0.9799 - val_loss: 0.7551 - val_accuracy: 0.8406\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 55s 184ms/step - loss: 0.0611 - accuracy: 0.9794 - val_loss: 0.8403 - val_accuracy: 0.8396\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 86.32075190544128\n",
      "\n",
      "        acc1       acc2       acc3       acc4      acc5       acc6      acc7  \\\n",
      "0  88.312912  88.689917  89.443922  87.747407  87.65316  89.349669  89.33962   \n",
      "\n",
      "        acc8       acc9      acc10        AVG  \n",
      "0  87.169814  86.698115  86.320752  88.072529  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJhKEZBiZT_V"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wHCrVIZfZT_W"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.312912</td>\n",
       "      <td>88.689917</td>\n",
       "      <td>89.443922</td>\n",
       "      <td>87.747407</td>\n",
       "      <td>87.65316</td>\n",
       "      <td>89.349669</td>\n",
       "      <td>89.33962</td>\n",
       "      <td>87.169814</td>\n",
       "      <td>86.698115</td>\n",
       "      <td>86.320752</td>\n",
       "      <td>88.072529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4      acc5       acc6      acc7  \\\n",
       "0  88.312912  88.689917  89.443922  87.747407  87.65316  89.349669  89.33962   \n",
       "\n",
       "        acc8       acc9      acc10        AVG  \n",
       "0  87.169814  86.698115  86.320752  88.072529  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iQesIbOMZT_W"
   },
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('GRU_MPQA_v2_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyAjy7JdZT_X"
   },
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_16oO2DSZT_X"
   },
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMSUb9KGZT_X"
   },
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ggn_LhMDZT_Y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "        # tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "YWXy4QWXZT_Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 100, 128)          140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 515,169\n",
      "Trainable params: 515,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yErgOnPWZT_Z"
   },
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "G9BJbOfuZT_Z"
   },
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=6, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "EzDaNOidZT_Z",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 112s 310ms/step - loss: 0.4151 - accuracy: 0.8125 - val_loss: 0.3366 - val_accuracy: 0.8643\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 69s 231ms/step - loss: 0.1785 - accuracy: 0.9357 - val_loss: 0.3543 - val_accuracy: 0.8633\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 79s 263ms/step - loss: 0.1214 - accuracy: 0.9584 - val_loss: 0.4109 - val_accuracy: 0.8558\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 79s 263ms/step - loss: 0.0865 - accuracy: 0.9686 - val_loss: 0.4483 - val_accuracy: 0.8549\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 83s 277ms/step - loss: 0.0660 - accuracy: 0.9761 - val_loss: 0.5224 - val_accuracy: 0.8520\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 82s 273ms/step - loss: 0.0464 - accuracy: 0.9813 - val_loss: 0.5542 - val_accuracy: 0.8492\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 81s 271ms/step - loss: 0.0438 - accuracy: 0.9843 - val_loss: 0.6150 - val_accuracy: 0.8445\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 86.42789721488953\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 96s 255ms/step - loss: 0.4082 - accuracy: 0.8152 - val_loss: 0.3146 - val_accuracy: 0.8794\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 62s 207ms/step - loss: 0.1872 - accuracy: 0.9323 - val_loss: 0.3478 - val_accuracy: 0.8699\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 65s 216ms/step - loss: 0.1205 - accuracy: 0.9554 - val_loss: 0.3901 - val_accuracy: 0.8690\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 63s 212ms/step - loss: 0.0881 - accuracy: 0.9664 - val_loss: 0.5000 - val_accuracy: 0.8746\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 63s 211ms/step - loss: 0.0602 - accuracy: 0.9757 - val_loss: 0.4970 - val_accuracy: 0.8690\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 63s 212ms/step - loss: 0.0488 - accuracy: 0.9811 - val_loss: 0.5713 - val_accuracy: 0.8728\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 63s 211ms/step - loss: 0.0437 - accuracy: 0.9829 - val_loss: 0.5955 - val_accuracy: 0.8671\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 87.93590664863586\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 109s 304ms/step - loss: 0.3996 - accuracy: 0.8189 - val_loss: 0.3127 - val_accuracy: 0.8709\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 86s 287ms/step - loss: 0.1812 - accuracy: 0.9385 - val_loss: 0.3221 - val_accuracy: 0.8812\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 92s 309ms/step - loss: 0.1296 - accuracy: 0.9539 - val_loss: 0.3649 - val_accuracy: 0.8643\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 94s 314ms/step - loss: 0.0814 - accuracy: 0.9683 - val_loss: 0.4056 - val_accuracy: 0.8662\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 92s 308ms/step - loss: 0.0594 - accuracy: 0.9770 - val_loss: 0.4345 - val_accuracy: 0.8615\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 95s 319ms/step - loss: 0.0498 - accuracy: 0.9792 - val_loss: 0.4806 - val_accuracy: 0.8690\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 93s 311ms/step - loss: 0.0420 - accuracy: 0.9838 - val_loss: 0.5289 - val_accuracy: 0.8662\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 95s 317ms/step - loss: 0.0386 - accuracy: 0.9831 - val_loss: 0.5582 - val_accuracy: 0.8596\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 88.12441229820251\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 90s 238ms/step - loss: 0.4081 - accuracy: 0.8095 - val_loss: 0.2981 - val_accuracy: 0.8878\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 57s 191ms/step - loss: 0.1924 - accuracy: 0.9308 - val_loss: 0.3027 - val_accuracy: 0.8841\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 58s 196ms/step - loss: 0.1298 - accuracy: 0.9561 - val_loss: 0.3176 - val_accuracy: 0.8794\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 58s 193ms/step - loss: 0.0901 - accuracy: 0.9651 - val_loss: 0.4119 - val_accuracy: 0.8746\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 58s 194ms/step - loss: 0.0713 - accuracy: 0.9716 - val_loss: 0.4181 - val_accuracy: 0.8794\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 57s 191ms/step - loss: 0.0547 - accuracy: 0.9785 - val_loss: 0.4723 - val_accuracy: 0.8784\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 57s 190ms/step - loss: 0.0449 - accuracy: 0.9820 - val_loss: 0.4966 - val_accuracy: 0.8699\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 88.78416419029236\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 151s 430ms/step - loss: 0.4177 - accuracy: 0.8075 - val_loss: 0.2840 - val_accuracy: 0.8822\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 120s 403ms/step - loss: 0.1891 - accuracy: 0.9313 - val_loss: 0.2844 - val_accuracy: 0.8869\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 122s 406ms/step - loss: 0.1228 - accuracy: 0.9572 - val_loss: 0.3094 - val_accuracy: 0.8897\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 120s 402ms/step - loss: 0.0952 - accuracy: 0.9648 - val_loss: 0.3594 - val_accuracy: 0.8907\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 121s 404ms/step - loss: 0.0668 - accuracy: 0.9744 - val_loss: 0.4203 - val_accuracy: 0.8822\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 122s 407ms/step - loss: 0.0509 - accuracy: 0.9799 - val_loss: 0.4391 - val_accuracy: 0.8812\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 122s 407ms/step - loss: 0.0486 - accuracy: 0.9810 - val_loss: 0.5303 - val_accuracy: 0.8718\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 122s 408ms/step - loss: 0.0421 - accuracy: 0.9802 - val_loss: 0.5417 - val_accuracy: 0.8765\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 122s 408ms/step - loss: 0.0410 - accuracy: 0.9808 - val_loss: 0.6153 - val_accuracy: 0.8728\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 121s 405ms/step - loss: 0.0314 - accuracy: 0.9876 - val_loss: 0.6496 - val_accuracy: 0.8690\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 89.06691670417786\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 138s 400ms/step - loss: 0.4127 - accuracy: 0.8113 - val_loss: 0.3231 - val_accuracy: 0.8822\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 114s 380ms/step - loss: 0.1890 - accuracy: 0.9314 - val_loss: 0.3464 - val_accuracy: 0.8718\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 109s 366ms/step - loss: 0.1272 - accuracy: 0.9564 - val_loss: 0.3674 - val_accuracy: 0.8624\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 101s 338ms/step - loss: 0.0938 - accuracy: 0.9661 - val_loss: 0.4095 - val_accuracy: 0.8633\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 101s 337ms/step - loss: 0.0678 - accuracy: 0.9750 - val_loss: 0.4602 - val_accuracy: 0.8520\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 101s 339ms/step - loss: 0.0521 - accuracy: 0.9783 - val_loss: 0.5238 - val_accuracy: 0.8615\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 103s 343ms/step - loss: 0.0465 - accuracy: 0.9806 - val_loss: 0.5945 - val_accuracy: 0.8530\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 88.21865916252136\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 106s 292ms/step - loss: 0.4142 - accuracy: 0.8105 - val_loss: 0.3018 - val_accuracy: 0.8868\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 81s 272ms/step - loss: 0.1840 - accuracy: 0.9339 - val_loss: 0.2969 - val_accuracy: 0.8896\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 88s 294ms/step - loss: 0.1144 - accuracy: 0.9607 - val_loss: 0.3384 - val_accuracy: 0.8613\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 88s 293ms/step - loss: 0.0952 - accuracy: 0.9651 - val_loss: 0.4184 - val_accuracy: 0.8604\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 87s 289ms/step - loss: 0.0660 - accuracy: 0.9747 - val_loss: 0.4772 - val_accuracy: 0.8613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40\n",
      "299/299 [==============================] - 88s 293ms/step - loss: 0.0514 - accuracy: 0.9776 - val_loss: 0.5102 - val_accuracy: 0.8642\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 89s 297ms/step - loss: 0.0418 - accuracy: 0.9826 - val_loss: 0.5776 - val_accuracy: 0.8660\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 88s 293ms/step - loss: 0.0385 - accuracy: 0.9821 - val_loss: 0.6697 - val_accuracy: 0.8604\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00008: early stopping\n",
      "Test Accuracy: 88.96226286888123\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 145s 423ms/step - loss: 0.3995 - accuracy: 0.8216 - val_loss: 0.3483 - val_accuracy: 0.8613\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 122s 409ms/step - loss: 0.1835 - accuracy: 0.9335 - val_loss: 0.4184 - val_accuracy: 0.8613\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 121s 406ms/step - loss: 0.1180 - accuracy: 0.9561 - val_loss: 0.3949 - val_accuracy: 0.8509\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 122s 408ms/step - loss: 0.0856 - accuracy: 0.9654 - val_loss: 0.4450 - val_accuracy: 0.8509\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 121s 404ms/step - loss: 0.0618 - accuracy: 0.9775 - val_loss: 0.5034 - val_accuracy: 0.8538\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 122s 407ms/step - loss: 0.0530 - accuracy: 0.9785 - val_loss: 0.5870 - val_accuracy: 0.8425\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 121s 405ms/step - loss: 0.0421 - accuracy: 0.9806 - val_loss: 0.6490 - val_accuracy: 0.8396\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 86.13207340240479\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 177s 526ms/step - loss: 0.4196 - accuracy: 0.8073 - val_loss: 0.3049 - val_accuracy: 0.8792\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 150s 502ms/step - loss: 0.1799 - accuracy: 0.9362 - val_loss: 0.3149 - val_accuracy: 0.8764\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 143s 477ms/step - loss: 0.1268 - accuracy: 0.9554 - val_loss: 0.3378 - val_accuracy: 0.8783\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 139s 463ms/step - loss: 0.0916 - accuracy: 0.9643 - val_loss: 0.3904 - val_accuracy: 0.8726\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 137s 458ms/step - loss: 0.0707 - accuracy: 0.9703 - val_loss: 0.4405 - val_accuracy: 0.8802\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 138s 461ms/step - loss: 0.0521 - accuracy: 0.9792 - val_loss: 0.4613 - val_accuracy: 0.8745\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 137s 459ms/step - loss: 0.0459 - accuracy: 0.9795 - val_loss: 0.5409 - val_accuracy: 0.8717\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 139s 464ms/step - loss: 0.0389 - accuracy: 0.9821 - val_loss: 0.5800 - val_accuracy: 0.8670\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 138s 463ms/step - loss: 0.0375 - accuracy: 0.9859 - val_loss: 0.6274 - val_accuracy: 0.8689\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 141s 473ms/step - loss: 0.0318 - accuracy: 0.9862 - val_loss: 0.6978 - val_accuracy: 0.8594\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 143s 477ms/step - loss: 0.0269 - accuracy: 0.9869 - val_loss: 0.6695 - val_accuracy: 0.8736\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n",
      "Test Accuracy: 88.01887035369873\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "299/299 [==============================] - 163s 484ms/step - loss: 0.4064 - accuracy: 0.8158 - val_loss: 0.2982 - val_accuracy: 0.8877\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 106s 355ms/step - loss: 0.1819 - accuracy: 0.9358 - val_loss: 0.3082 - val_accuracy: 0.8783\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 110s 369ms/step - loss: 0.1240 - accuracy: 0.9580 - val_loss: 0.3581 - val_accuracy: 0.8698\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 110s 366ms/step - loss: 0.0891 - accuracy: 0.9657 - val_loss: 0.4095 - val_accuracy: 0.8689\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 110s 367ms/step - loss: 0.0591 - accuracy: 0.9787 - val_loss: 0.4643 - val_accuracy: 0.8689\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 108s 362ms/step - loss: 0.0513 - accuracy: 0.9796 - val_loss: 0.4750 - val_accuracy: 0.8585\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 110s 368ms/step - loss: 0.0465 - accuracy: 0.9811 - val_loss: 0.5501 - val_accuracy: 0.8623\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n",
      "Test Accuracy: 88.77358436584473\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  86.427897  87.935907  88.124412  88.784164  89.066917  88.218659   \n",
      "\n",
      "        acc7       acc8      acc9      acc10        AVG  \n",
      "0  88.962263  86.132073  88.01887  88.773584  88.044475  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUZ-RMhVZT_a"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "XQjCT-UaZT_a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.427897</td>\n",
       "      <td>87.935907</td>\n",
       "      <td>88.124412</td>\n",
       "      <td>88.784164</td>\n",
       "      <td>89.066917</td>\n",
       "      <td>88.218659</td>\n",
       "      <td>88.962263</td>\n",
       "      <td>86.132073</td>\n",
       "      <td>88.01887</td>\n",
       "      <td>88.773584</td>\n",
       "      <td>88.044475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  86.427897  87.935907  88.124412  88.784164  89.066917  88.218659   \n",
       "\n",
       "        acc7       acc8      acc9      acc10        AVG  \n",
       "0  88.962263  86.132073  88.01887  88.773584  88.044475  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "FL_qi5_LZT_b"
   },
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('GRU_MPQA_v2_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "XJhKEZBiZT_V",
    "fUZ-RMhVZT_a"
   ],
   "name": "GRU_MPQA_v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
