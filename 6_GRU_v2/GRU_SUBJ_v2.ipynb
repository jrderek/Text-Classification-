{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Classification with SUBJ Dataset\n",
    "<hr>\n",
    "\n",
    "We will build a text classification model using GRU model on the SUBJ Dataset. Since there is no standard train/test split for this dataset, we will use 10-Fold Cross Validation (CV). \n",
    "\n",
    "## Load the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "%config IPCompleter.use_jedi=False\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>smart and alert , thirteen conversations about...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>color , musical bounce and warm seas lapping o...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is not a mass market entertainment but an u...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a light hearted french film about the spiritua...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my wife is an actress has its moments in looki...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>in the end , they discover that balance in lif...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>a counterfeit 1000 tomin bank note is passed i...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>enter the beautiful and mysterious secret agen...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>after listening to a missionary from china spe...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>looking for a short cut to fame , glass concoc...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label  split\n",
       "0     smart and alert , thirteen conversations about...      0  train\n",
       "1     color , musical bounce and warm seas lapping o...      0  train\n",
       "2     it is not a mass market entertainment but an u...      0  train\n",
       "3     a light hearted french film about the spiritua...      0  train\n",
       "4     my wife is an actress has its moments in looki...      0  train\n",
       "...                                                 ...    ...    ...\n",
       "9995  in the end , they discover that balance in lif...      1  train\n",
       "9996  a counterfeit 1000 tomin bank note is passed i...      1  train\n",
       "9997  enter the beautiful and mysterious secret agen...      1  train\n",
       "9998  after listening to a missionary from china spe...      1  train\n",
       "9999  looking for a short cut to fame , glass concoc...      1  train\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_pickle('../../../0_data/SUBJ/SUBJ.pkl')\n",
    "corpus.label = corpus.label.astype(int)\n",
    "print(corpus.shape)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  10000 non-null  object\n",
      " 1   label     10000 non-null  int32 \n",
      " 2   split     10000 non-null  object\n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 195.4+ KB\n"
     ]
    }
   ],
   "source": [
    "corpus.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence  split\n",
       "label                 \n",
       "0          5000   5000\n",
       "1          5000   5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby( by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smart and alert , thirteen conversations about one thing is a small gem .'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--## Split Dataset-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "<hr>\n",
    "\n",
    "Preparing data for word embedding, especially for pre-trained word embedding like Word2Vec or GloVe, __don't use standard preprocessing steps like stemming or stopword removal__. Compared to our approach on cleaning the text when doing word count based feature extraction (e.g. TFIDF) such as removing stopwords, stemming etc, now we will keep these words as we do not want to lose such information that might help the model learn better.\n",
    "\n",
    "__Tomas Mikolov__, one of the developers of Word2Vec, in _word2vec-toolkit: google groups thread., 2015_, suggests only very minimal text cleaning is required when learning a word embedding model. Sometimes, it's good to disconnect\n",
    "In short, what we will do is:\n",
    "- Puntuations removal\n",
    "- Lower the letter case\n",
    "- Tokenization\n",
    "\n",
    "The process above will be handled by __Tokenizer__ class in TensorFlow\n",
    "\n",
    "- <b>One way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the max length of sequence\n",
    "def max_length(sequences):\n",
    "    '''\n",
    "    input:\n",
    "        sequences: a 2D list of integer sequences\n",
    "    output:\n",
    "        max_length: the max length of the sequences\n",
    "    '''\n",
    "    max_length = 0\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        if max_length < length:\n",
    "            max_length = length\n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of sentence:  my wife is an actress has its moments in looking at the comic effects of jealousy . in the end , though , it is only mildly amusing when it could have been so much more .\n",
      "Into a sequence of int: [336, 208, 8, 16, 921, 25, 29, 312, 7, 313, 32, 2, 488, 551, 5, 3203, 7, 2, 129, 194, 10, 8, 60, 2330, 716, 39, 10, 128, 43, 82, 54, 81, 45]\n",
      "Into a padded sequence: [ 336  208    8   16  921   25   29  312    7  313   32    2  488  551\n",
      "    5 3203    7    2  129  194   10    8   60 2330  716   39   10  128\n",
      "   43   82   54   81   45    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "print(\"Example of sentence: \", sentences[4])\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn the text into sequence\n",
    "training_sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = max_length(training_sequences)\n",
    "\n",
    "print('Into a sequence of int:', training_sequences[4])\n",
    "\n",
    "# Pad the sequence to have the same size\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "print('Into a padded sequence:', training_padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK> 1\n",
      "the 2\n",
      "a 3\n",
      "and 4\n",
      "of 5\n",
      "to 6\n",
      "in 7\n",
      "is 8\n",
      "'s 9\n",
      "it 10\n",
      "21324\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "# See the first 10 words in the vocabulary\n",
    "for i, word in enumerate(word_index):\n",
    "    print(word, word_index.get(word))\n",
    "    if i==9:\n",
    "        break\n",
    "vocab_size = len(word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Embedding Random\n",
    "<hr>\n",
    "\n",
    "<img src=\"model.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model(input_dim = None, output_dim=300, max_length = None ):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, )),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 487,009\n",
      "Trainable params: 487,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model( input_dim=1000, max_length=100)\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=5, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 84s 257ms/step - loss: 0.4351 - accuracy: 0.7757 - val_loss: 0.2216 - val_accuracy: 0.9150\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 70s 248ms/step - loss: 0.0813 - accuracy: 0.9736 - val_loss: 0.2411 - val_accuracy: 0.9010\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 74s 264ms/step - loss: 0.0211 - accuracy: 0.9952 - val_loss: 0.3469 - val_accuracy: 0.9070\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 71s 253ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.4108 - val_accuracy: 0.8900\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 75s 267ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.4150 - val_accuracy: 0.9140\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 79s 279ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.4509 - val_accuracy: 0.9040\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.50000214576721\n",
      "Training 2: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 125s 401ms/step - loss: 0.4314 - accuracy: 0.7902 - val_loss: 0.2182 - val_accuracy: 0.9160\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 89s 314ms/step - loss: 0.0817 - accuracy: 0.9765 - val_loss: 0.2644 - val_accuracy: 0.9020\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 110s 390ms/step - loss: 0.0197 - accuracy: 0.9955 - val_loss: 0.3410 - val_accuracy: 0.8910\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 104s 370ms/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.4430 - val_accuracy: 0.8890\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 101s 360ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.5179 - val_accuracy: 0.8910\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 105s 372ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.3471 - val_accuracy: 0.9000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.60000085830688\n",
      "Training 3: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 114s 361ms/step - loss: 0.4342 - accuracy: 0.7942 - val_loss: 0.2109 - val_accuracy: 0.9220\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 86s 305ms/step - loss: 0.0702 - accuracy: 0.9753 - val_loss: 0.2407 - val_accuracy: 0.9210\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 96s 341ms/step - loss: 0.0179 - accuracy: 0.9946 - val_loss: 0.3362 - val_accuracy: 0.9030\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 101s 360ms/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.3528 - val_accuracy: 0.9140\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 105s 373ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 0.3192 - val_accuracy: 0.9050\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 106s 377ms/step - loss: 0.0111 - accuracy: 0.9955 - val_loss: 0.3718 - val_accuracy: 0.9080\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.1999990940094\n",
      "Training 4: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 165s 547ms/step - loss: 0.4353 - accuracy: 0.7873 - val_loss: 0.2183 - val_accuracy: 0.9060\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 136s 482ms/step - loss: 0.0787 - accuracy: 0.9751 - val_loss: 0.2968 - val_accuracy: 0.8960\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 151s 537ms/step - loss: 0.0159 - accuracy: 0.9966 - val_loss: 0.3515 - val_accuracy: 0.8950\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 149s 527ms/step - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.3500 - val_accuracy: 0.8840\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 144s 510ms/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.4359 - val_accuracy: 0.8880\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 143s 509ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.5749 - val_accuracy: 0.8860\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 90.6000018119812\n",
      "Training 5: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 119s 385ms/step - loss: 0.4343 - accuracy: 0.7841 - val_loss: 0.1955 - val_accuracy: 0.9290\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 165s 586ms/step - loss: 0.0716 - accuracy: 0.9769 - val_loss: 0.2045 - val_accuracy: 0.9280\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 160s 567ms/step - loss: 0.0202 - accuracy: 0.9952 - val_loss: 0.3264 - val_accuracy: 0.9120\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 160s 567ms/step - loss: 0.0098 - accuracy: 0.9973 - val_loss: 0.2563 - val_accuracy: 0.9020\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 157s 557ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.3456 - val_accuracy: 0.9110\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 163s 577ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.3827 - val_accuracy: 0.9160\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.90000200271606\n",
      "Training 6: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 132s 429ms/step - loss: 0.4296 - accuracy: 0.8008 - val_loss: 0.2507 - val_accuracy: 0.9040\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 88s 311ms/step - loss: 0.0673 - accuracy: 0.9820 - val_loss: 0.3018 - val_accuracy: 0.8980\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 90s 319ms/step - loss: 0.0257 - accuracy: 0.9938 - val_loss: 0.4001 - val_accuracy: 0.8940\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 90s 318ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.4873 - val_accuracy: 0.8850\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 90s 318ms/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.4236 - val_accuracy: 0.8940\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 90s 319ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5670 - val_accuracy: 0.8910\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 90.39999842643738\n",
      "Training 7: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 145s 475ms/step - loss: 0.4437 - accuracy: 0.7881 - val_loss: 0.2203 - val_accuracy: 0.9100\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 111s 392ms/step - loss: 0.0771 - accuracy: 0.9726 - val_loss: 0.2393 - val_accuracy: 0.9040\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 117s 414ms/step - loss: 0.0289 - accuracy: 0.9917 - val_loss: 0.3564 - val_accuracy: 0.9000\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 110s 390ms/step - loss: 0.0130 - accuracy: 0.9970 - val_loss: 0.3713 - val_accuracy: 0.9070\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 112s 398ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.4522 - val_accuracy: 0.8980\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 113s 400ms/step - loss: 6.4205e-04 - accuracy: 1.0000 - val_loss: 0.5230 - val_accuracy: 0.8930\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 91.00000262260437\n",
      "Training 8: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 74s 230ms/step - loss: 0.4274 - accuracy: 0.7937 - val_loss: 0.1909 - val_accuracy: 0.9350\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 172s 611ms/step - loss: 0.0787 - accuracy: 0.9767 - val_loss: 0.2146 - val_accuracy: 0.9250\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 174s 616ms/step - loss: 0.0247 - accuracy: 0.9924 - val_loss: 0.2331 - val_accuracy: 0.9310\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 173s 614ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.2835 - val_accuracy: 0.9210\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 177s 628ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.2969 - val_accuracy: 0.9220\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 177s 628ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.2769 - val_accuracy: 0.9170\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 93.50000023841858\n",
      "Training 9: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 178s 593ms/step - loss: 0.4390 - accuracy: 0.7900 - val_loss: 0.2229 - val_accuracy: 0.9250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "282/282 [==============================] - 139s 491ms/step - loss: 0.0793 - accuracy: 0.9754 - val_loss: 0.2832 - val_accuracy: 0.9080\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 134s 475ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.3074 - val_accuracy: 0.9020\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 132s 467ms/step - loss: 0.0067 - accuracy: 0.9987 - val_loss: 0.3414 - val_accuracy: 0.9070\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 130s 463ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 0.4280 - val_accuracy: 0.8930\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 130s 462ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.4531 - val_accuracy: 0.8860\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.5000011920929\n",
      "Training 10: \n",
      "Epoch 1/15\n",
      "282/282 [==============================] - 113s 365ms/step - loss: 0.4532 - accuracy: 0.7945 - val_loss: 0.2112 - val_accuracy: 0.9240\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 70s 250ms/step - loss: 0.0825 - accuracy: 0.9711 - val_loss: 0.2547 - val_accuracy: 0.9090\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 74s 263ms/step - loss: 0.0216 - accuracy: 0.9941 - val_loss: 0.2866 - val_accuracy: 0.9120\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 74s 264ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.3517 - val_accuracy: 0.9160\n",
      "Epoch 5/15\n",
      "282/282 [==============================] - 73s 260ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.3830 - val_accuracy: 0.9220\n",
      "Epoch 6/15\n",
      "282/282 [==============================] - 72s 257ms/step - loss: 4.4777e-04 - accuracy: 1.0000 - val_loss: 0.4091 - val_accuracy: 0.9140\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n",
      "Test Accuracy: 92.40000247955322\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  91.500002  91.600001  92.199999  90.600002  92.900002  90.399998   \n",
      "\n",
      "        acc7  acc8       acc9      acc10        AVG  \n",
      "0  91.000003  93.5  92.500001  92.400002  91.860001  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model(input_dim=vocab_size, max_length=max_len)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=15, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record = record.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.500002</td>\n",
       "      <td>91.600001</td>\n",
       "      <td>92.199999</td>\n",
       "      <td>90.600002</td>\n",
       "      <td>92.900002</td>\n",
       "      <td>90.399998</td>\n",
       "      <td>91.000003</td>\n",
       "      <td>93.5</td>\n",
       "      <td>92.500001</td>\n",
       "      <td>92.400002</td>\n",
       "      <td>91.860001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  91.500002  91.600001  92.199999  90.600002  92.900002  90.399998   \n",
       "\n",
       "        acc7  acc8       acc9      acc10        AVG  \n",
       "0  91.000003  93.5  92.500001  92.400002  91.860001  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record\n",
    "report = report.to_excel('GRU_SUBJ_v2.xlsx', sheet_name='random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Word2Vec Static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using and updating pre-trained embeddings__\n",
    "* In this part, we will create an Embedding layer in Tensorflow Keras using a pre-trained word embedding called Word2Vec 300-d tht has been trained 100 bilion words from Google News.\n",
    "* In this part,  we will leave the embeddings fixed instead of updating them (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Load `Word2Vec` Pre-trained Word Embedding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word2vec = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.64062500e-01,  1.87500000e-01, -4.10156250e-02,  1.25000000e-01,\n",
       "       -3.22265625e-02,  8.69140625e-02,  1.19140625e-01, -1.26953125e-01,\n",
       "        1.77001953e-02,  8.83789062e-02,  2.12402344e-02, -2.00195312e-01,\n",
       "        4.83398438e-02, -1.01074219e-01, -1.89453125e-01,  2.30712891e-02,\n",
       "        1.17675781e-01,  7.51953125e-02, -8.39843750e-02, -1.33666992e-02,\n",
       "        1.53320312e-01,  4.08203125e-01,  3.80859375e-02,  3.36914062e-02,\n",
       "       -4.02832031e-02, -6.88476562e-02,  9.03320312e-02,  2.12890625e-01,\n",
       "        1.72119141e-02, -6.44531250e-02, -1.29882812e-01,  1.40625000e-01,\n",
       "        2.38281250e-01,  1.37695312e-01, -1.76757812e-01, -2.71484375e-01,\n",
       "       -1.36718750e-01, -1.69921875e-01, -9.15527344e-03,  3.47656250e-01,\n",
       "        2.22656250e-01, -3.06640625e-01,  1.98242188e-01,  1.33789062e-01,\n",
       "       -4.34570312e-02, -5.12695312e-02, -3.46679688e-02, -8.49609375e-02,\n",
       "        1.01562500e-01,  1.42578125e-01, -7.95898438e-02,  1.78710938e-01,\n",
       "        2.30468750e-01,  3.90625000e-02,  8.69140625e-02,  2.40234375e-01,\n",
       "       -7.61718750e-02,  8.64257812e-02,  1.02539062e-01,  2.64892578e-02,\n",
       "       -6.88476562e-02, -9.70458984e-03, -2.77343750e-01, -1.73828125e-01,\n",
       "        5.10253906e-02,  1.89208984e-02, -2.09960938e-01, -1.14257812e-01,\n",
       "       -2.81982422e-02,  7.81250000e-02,  2.01463699e-05,  5.76782227e-03,\n",
       "        2.38281250e-01,  2.55126953e-02, -3.41796875e-01,  2.23632812e-01,\n",
       "        2.48046875e-01,  1.61132812e-01, -7.95898438e-02,  2.55859375e-01,\n",
       "        5.46875000e-02, -1.19628906e-01,  2.81982422e-02,  2.13623047e-02,\n",
       "       -8.60595703e-03,  4.66308594e-02, -2.78320312e-02,  2.98828125e-01,\n",
       "       -1.82617188e-01,  2.42187500e-01, -7.37304688e-02,  7.81250000e-02,\n",
       "       -2.63671875e-01, -1.73828125e-01,  3.14941406e-02,  1.67968750e-01,\n",
       "       -6.39648438e-02,  1.69677734e-02,  4.68750000e-02, -1.64062500e-01,\n",
       "       -2.94921875e-01, -3.23486328e-03, -1.60156250e-01, -1.39648438e-01,\n",
       "       -8.78906250e-02, -1.47460938e-01,  9.71679688e-02, -1.60156250e-01,\n",
       "        3.36914062e-02, -1.18164062e-01, -2.28515625e-01, -9.08203125e-02,\n",
       "       -8.34960938e-02, -8.74023438e-02,  2.09960938e-01, -1.67968750e-01,\n",
       "        1.60156250e-01,  7.91015625e-02, -1.03515625e-01, -1.22558594e-01,\n",
       "       -1.39648438e-01,  2.99072266e-02,  5.00488281e-02, -4.46777344e-02,\n",
       "       -4.12597656e-02, -1.94335938e-01,  6.15234375e-02,  2.47070312e-01,\n",
       "        5.24902344e-02, -1.18164062e-01,  4.68750000e-02,  1.79290771e-03,\n",
       "        2.57812500e-01,  2.65625000e-01, -4.15039062e-02,  1.75781250e-01,\n",
       "        2.25830078e-02, -2.14843750e-02, -4.10156250e-02,  6.88476562e-02,\n",
       "        1.87500000e-01, -8.34960938e-02,  4.39453125e-02, -1.66015625e-01,\n",
       "        8.00781250e-02,  1.52343750e-01,  7.65991211e-03, -3.66210938e-02,\n",
       "        1.87988281e-02, -2.69531250e-01, -3.88183594e-02,  1.65039062e-01,\n",
       "       -8.85009766e-03,  3.37890625e-01, -2.63671875e-01, -1.63574219e-02,\n",
       "        8.20312500e-02, -2.17773438e-01, -1.14746094e-01,  9.57031250e-02,\n",
       "       -6.07910156e-02, -1.51367188e-01,  7.61718750e-02,  7.27539062e-02,\n",
       "        7.22656250e-02, -1.70898438e-02,  3.34472656e-02,  2.27539062e-01,\n",
       "        1.42578125e-01,  1.21093750e-01, -1.83593750e-01,  1.02050781e-01,\n",
       "        6.83593750e-02,  1.28906250e-01, -1.28784180e-02,  1.63085938e-01,\n",
       "        2.83203125e-02, -6.73828125e-02, -3.53515625e-01, -1.60980225e-03,\n",
       "       -4.17480469e-02, -2.87109375e-01,  3.75976562e-02, -1.20117188e-01,\n",
       "        7.08007812e-02,  2.56347656e-02,  5.66406250e-02,  1.14746094e-02,\n",
       "       -1.69921875e-01, -1.16577148e-02, -4.73632812e-02,  1.94335938e-01,\n",
       "        3.61328125e-02, -1.21093750e-01, -4.02832031e-02,  1.25000000e-01,\n",
       "       -4.44335938e-02, -1.10351562e-01, -8.30078125e-02, -6.59179688e-02,\n",
       "       -1.55029297e-02,  1.59179688e-01, -1.87500000e-01, -3.17382812e-02,\n",
       "        8.34960938e-02, -1.23535156e-01, -1.68945312e-01, -2.81250000e-01,\n",
       "       -1.50390625e-01,  9.47265625e-02, -2.53906250e-01,  1.04003906e-01,\n",
       "        1.07421875e-01, -2.70080566e-03,  1.42211914e-02, -1.01074219e-01,\n",
       "        3.61328125e-02, -6.64062500e-02, -2.73437500e-01, -1.17187500e-02,\n",
       "       -9.52148438e-02,  2.23632812e-01,  1.28906250e-01, -1.24511719e-01,\n",
       "       -2.57568359e-02,  3.12500000e-01, -6.93359375e-02, -1.57226562e-01,\n",
       "       -1.91406250e-01,  6.44531250e-02, -1.64062500e-01,  1.70898438e-02,\n",
       "       -1.02050781e-01, -2.30468750e-01,  2.12890625e-01, -4.41894531e-02,\n",
       "       -2.20703125e-01, -7.51953125e-02,  2.79296875e-01,  2.45117188e-01,\n",
       "        2.04101562e-01,  1.50390625e-01,  1.36718750e-01, -1.49414062e-01,\n",
       "       -1.79687500e-01,  1.10839844e-01, -8.10546875e-02, -1.22558594e-01,\n",
       "       -4.58984375e-02, -2.07031250e-01, -1.48437500e-01,  2.79296875e-01,\n",
       "        2.28515625e-01,  2.11914062e-01,  1.30859375e-01, -3.51562500e-02,\n",
       "        2.09960938e-01, -6.34765625e-02, -1.15722656e-01, -2.05078125e-01,\n",
       "        1.26953125e-01, -2.11914062e-01, -2.55859375e-01, -1.57470703e-02,\n",
       "        1.16699219e-01, -1.30004883e-02, -1.07910156e-01, -3.39843750e-01,\n",
       "        1.54296875e-01, -1.71875000e-01, -2.28271484e-02,  6.44531250e-02,\n",
       "        3.78906250e-01,  1.62109375e-01,  5.17578125e-02, -8.78906250e-02,\n",
       "       -1.78222656e-02, -4.58984375e-02, -2.06054688e-01,  6.59179688e-02,\n",
       "        2.26562500e-01,  1.34765625e-01,  1.03515625e-01,  2.64892578e-02,\n",
       "        1.97265625e-01, -9.47265625e-02, -7.71484375e-02,  1.04003906e-01,\n",
       "        9.71679688e-02, -1.41601562e-01,  1.17187500e-02,  1.97265625e-01,\n",
       "        3.61633301e-03,  2.53906250e-01, -1.30004883e-02,  3.46679688e-02,\n",
       "        1.73339844e-02,  1.08886719e-01, -1.01928711e-02,  2.07519531e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the dense vector value for the word 'handsome'\n",
    "# word2vec.word_vec('handsome') # 0.11376953\n",
    "word2vec.word_vec('cool') # 1.64062500e-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Check number of training words present in Word2Vec__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_words_in_word2vector(word_to_vec_map, word_to_index):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    count = 0\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            count+=1\n",
    "            \n",
    "    return print('Found {} words present from {} training vocabulary in the set of pre-trained word vector'.format(count, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17913 words present from 21324 training vocabulary in the set of pre-trained word vector\n"
     ]
    }
   ],
   "source": [
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "# Cleaning and Tokenization\n",
    "tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "training_words_in_word2vector(word2vec, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Define a `pretrained_embedding_layer` function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_mean:  -0.003527845\n",
      "emb_std:  0.13315111\n"
     ]
    }
   ],
   "source": [
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()\n",
    "print('emb_mean: ', emb_mean)\n",
    "print('emb_std: ', emb_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
    "    '''\n",
    "    input:\n",
    "        word_to_vec_map: a word2vec GoogleNews-vectors-negative300.bin model loaded using gensim.models\n",
    "        word_to_index: word to index mapping from training set\n",
    "    '''\n",
    "    np.random.seed(2021)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.word_vec('handsome').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution values\n",
    "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19468211,  0.08648376, -0.05924511, ..., -0.16683994,\n",
       "        -0.09975549, -0.08595189],\n",
       "       [-0.13509196, -0.07441947,  0.15388953, ..., -0.05400787,\n",
       "        -0.13156594, -0.05996158],\n",
       "       [ 0.11376953,  0.1796875 , -0.265625  , ..., -0.21875   ,\n",
       "        -0.03930664,  0.20996094],\n",
       "       [ 0.1640625 ,  0.1875    , -0.04101562, ...,  0.10888672,\n",
       "        -0.01019287,  0.02075195],\n",
       "       [ 0.10888672, -0.16699219,  0.08984375, ..., -0.19628906,\n",
       "        -0.23144531,  0.04614258]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "w_2_i = {'<UNK>': 1, 'handsome': 2, 'cool': 3, 'shit': 4 }\n",
    "em_matrix = pretrained_embedding_matrix(word2vec, w_2_i, emb_mean, emb_std)\n",
    "em_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_2(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = False),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 128)          140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 523,361\n",
      "Trainable params: 223,361\n",
      "Non-trainable params: 300,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_2( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') >= 0.9):\n",
    "            print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=8, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 126s 366ms/step - loss: 0.3842 - accuracy: 0.8334 - val_loss: 0.2478 - val_accuracy: 0.9100\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 90s 321ms/step - loss: 0.2153 - accuracy: 0.9181 - val_loss: 0.2301 - val_accuracy: 0.9080\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 102s 360ms/step - loss: 0.1838 - accuracy: 0.9319 - val_loss: 0.2335 - val_accuracy: 0.8990\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 102s 360ms/step - loss: 0.1442 - accuracy: 0.9464 - val_loss: 0.2594 - val_accuracy: 0.8970\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 95s 336ms/step - loss: 0.1208 - accuracy: 0.9566 - val_loss: 0.2375 - val_accuracy: 0.9090\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 88s 313ms/step - loss: 0.0968 - accuracy: 0.9622 - val_loss: 0.3644 - val_accuracy: 0.8950\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 88s 314ms/step - loss: 0.0683 - accuracy: 0.9755 - val_loss: 0.2705 - val_accuracy: 0.9040\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 83s 294ms/step - loss: 0.0465 - accuracy: 0.9841 - val_loss: 0.4236 - val_accuracy: 0.9090\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 76s 268ms/step - loss: 0.0366 - accuracy: 0.9881 - val_loss: 0.4313 - val_accuracy: 0.9030\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 91.00000262260437\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 147s 444ms/step - loss: 0.4067 - accuracy: 0.8069 - val_loss: 0.2177 - val_accuracy: 0.9190\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 112s 398ms/step - loss: 0.2097 - accuracy: 0.9201 - val_loss: 0.1947 - val_accuracy: 0.9250\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 135s 480ms/step - loss: 0.1785 - accuracy: 0.9287 - val_loss: 0.1897 - val_accuracy: 0.9300\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 131s 465ms/step - loss: 0.1422 - accuracy: 0.9442 - val_loss: 0.2368 - val_accuracy: 0.9110\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 132s 468ms/step - loss: 0.1288 - accuracy: 0.9483 - val_loss: 0.2223 - val_accuracy: 0.9150\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 129s 459ms/step - loss: 0.1026 - accuracy: 0.9629 - val_loss: 0.2082 - val_accuracy: 0.9330\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 137s 488ms/step - loss: 0.0863 - accuracy: 0.9637 - val_loss: 0.2205 - val_accuracy: 0.9270\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 136s 481ms/step - loss: 0.0577 - accuracy: 0.9794 - val_loss: 0.2450 - val_accuracy: 0.9250\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 133s 471ms/step - loss: 0.0368 - accuracy: 0.9860 - val_loss: 0.3097 - val_accuracy: 0.9210\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 125s 443ms/step - loss: 0.0284 - accuracy: 0.9893 - val_loss: 0.3394 - val_accuracy: 0.9270\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 128s 454ms/step - loss: 0.0274 - accuracy: 0.9919 - val_loss: 0.3284 - val_accuracy: 0.9190\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 128s 455ms/step - loss: 0.0187 - accuracy: 0.9925 - val_loss: 0.3838 - val_accuracy: 0.9200\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 130s 461ms/step - loss: 0.0059 - accuracy: 0.9979 - val_loss: 0.5429 - val_accuracy: 0.9280\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 127s 450ms/step - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.5502 - val_accuracy: 0.9040\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00014: early stopping\n",
      "Test Accuracy: 93.30000281333923\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 103s 296ms/step - loss: 0.3904 - accuracy: 0.8243 - val_loss: 0.2763 - val_accuracy: 0.8800\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 76s 271ms/step - loss: 0.2153 - accuracy: 0.9183 - val_loss: 0.2941 - val_accuracy: 0.8730\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 82s 292ms/step - loss: 0.1762 - accuracy: 0.9327 - val_loss: 0.2485 - val_accuracy: 0.8990\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 81s 288ms/step - loss: 0.1440 - accuracy: 0.9437 - val_loss: 0.2286 - val_accuracy: 0.9090\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 80s 284ms/step - loss: 0.1109 - accuracy: 0.9602 - val_loss: 0.2659 - val_accuracy: 0.8980\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 82s 292ms/step - loss: 0.0928 - accuracy: 0.9620 - val_loss: 0.2831 - val_accuracy: 0.9040\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 84s 296ms/step - loss: 0.0744 - accuracy: 0.9721 - val_loss: 0.3988 - val_accuracy: 0.8990\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 82s 291ms/step - loss: 0.0465 - accuracy: 0.9840 - val_loss: 0.3552 - val_accuracy: 0.9030\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 82s 290ms/step - loss: 0.0265 - accuracy: 0.9907 - val_loss: 0.4732 - val_accuracy: 0.8990\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 81s 289ms/step - loss: 0.0233 - accuracy: 0.9915 - val_loss: 0.4912 - val_accuracy: 0.8970\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 83s 294ms/step - loss: 0.0217 - accuracy: 0.9920 - val_loss: 0.6484 - val_accuracy: 0.8840\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 83s 293ms/step - loss: 0.0189 - accuracy: 0.9930 - val_loss: 0.5215 - val_accuracy: 0.9000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 90.89999794960022\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 195s 601ms/step - loss: 0.3900 - accuracy: 0.8245 - val_loss: 0.2459 - val_accuracy: 0.9160\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 126s 447ms/step - loss: 0.2044 - accuracy: 0.9186 - val_loss: 0.2462 - val_accuracy: 0.9030\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 139s 493ms/step - loss: 0.1820 - accuracy: 0.9306 - val_loss: 0.2291 - val_accuracy: 0.9180\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 139s 494ms/step - loss: 0.1404 - accuracy: 0.9429 - val_loss: 0.2850 - val_accuracy: 0.8950\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 138s 489ms/step - loss: 0.1211 - accuracy: 0.9515 - val_loss: 0.2528 - val_accuracy: 0.9180\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 137s 486ms/step - loss: 0.0943 - accuracy: 0.9647 - val_loss: 0.2678 - val_accuracy: 0.9130\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 137s 487ms/step - loss: 0.0712 - accuracy: 0.9723 - val_loss: 0.2848 - val_accuracy: 0.9210\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 141s 500ms/step - loss: 0.0478 - accuracy: 0.9824 - val_loss: 0.3501 - val_accuracy: 0.9190\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 132s 470ms/step - loss: 0.0359 - accuracy: 0.9868 - val_loss: 0.3956 - val_accuracy: 0.9090\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 140s 497ms/step - loss: 0.0400 - accuracy: 0.9857 - val_loss: 0.4208 - val_accuracy: 0.9130\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 132s 469ms/step - loss: 0.0149 - accuracy: 0.9949 - val_loss: 0.4449 - val_accuracy: 0.9090\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 131s 463ms/step - loss: 0.0146 - accuracy: 0.9959 - val_loss: 0.5659 - val_accuracy: 0.9010\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 139s 492ms/step - loss: 0.0097 - accuracy: 0.9967 - val_loss: 0.5510 - val_accuracy: 0.9090\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 138s 488ms/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 0.5014 - val_accuracy: 0.9280\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 134s 476ms/step - loss: 0.0117 - accuracy: 0.9963 - val_loss: 0.5830 - val_accuracy: 0.9110\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 136s 482ms/step - loss: 0.0279 - accuracy: 0.9930 - val_loss: 0.6605 - val_accuracy: 0.9240\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 137s 486ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.6296 - val_accuracy: 0.9150\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 140s 497ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.7163 - val_accuracy: 0.9170\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 126s 445ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.6163 - val_accuracy: 0.9210\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 131s 465ms/step - loss: 0.0080 - accuracy: 0.9970 - val_loss: 0.6929 - val_accuracy: 0.9210\n",
      "Epoch 21/40\n",
      "282/282 [==============================] - 126s 446ms/step - loss: 0.0112 - accuracy: 0.9967 - val_loss: 0.6916 - val_accuracy: 0.9160\n",
      "Epoch 22/40\n",
      "282/282 [==============================] - 131s 466ms/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.7124 - val_accuracy: 0.9160\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00022: early stopping\n",
      "Test Accuracy: 92.79999732971191\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 121s 361ms/step - loss: 0.3874 - accuracy: 0.8293 - val_loss: 0.2814 - val_accuracy: 0.8900\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 99s 352ms/step - loss: 0.2235 - accuracy: 0.9114 - val_loss: 0.2119 - val_accuracy: 0.9120\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 101s 358ms/step - loss: 0.1783 - accuracy: 0.9314 - val_loss: 0.1889 - val_accuracy: 0.9280\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 101s 357ms/step - loss: 0.1479 - accuracy: 0.9438 - val_loss: 0.1871 - val_accuracy: 0.9190\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 101s 357ms/step - loss: 0.1212 - accuracy: 0.9531 - val_loss: 0.2092 - val_accuracy: 0.9270\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 100s 355ms/step - loss: 0.0945 - accuracy: 0.9687 - val_loss: 0.2195 - val_accuracy: 0.9260\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 100s 354ms/step - loss: 0.0658 - accuracy: 0.9765 - val_loss: 0.2641 - val_accuracy: 0.9270\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 96s 342ms/step - loss: 0.0676 - accuracy: 0.9787 - val_loss: 0.2630 - val_accuracy: 0.9270\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 99s 349ms/step - loss: 0.0404 - accuracy: 0.9840 - val_loss: 0.2867 - val_accuracy: 0.9330\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 98s 348ms/step - loss: 0.0314 - accuracy: 0.9889 - val_loss: 0.3614 - val_accuracy: 0.9230\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 99s 351ms/step - loss: 0.0230 - accuracy: 0.9922 - val_loss: 0.3569 - val_accuracy: 0.9300\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 99s 350ms/step - loss: 0.0186 - accuracy: 0.9936 - val_loss: 0.3718 - val_accuracy: 0.9220\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 98s 347ms/step - loss: 0.0175 - accuracy: 0.9943 - val_loss: 0.3794 - val_accuracy: 0.9310\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 97s 346ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.4862 - val_accuracy: 0.9340\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 98s 347ms/step - loss: 0.0141 - accuracy: 0.9952 - val_loss: 0.4483 - val_accuracy: 0.9270\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 97s 345ms/step - loss: 0.0135 - accuracy: 0.9950 - val_loss: 0.4436 - val_accuracy: 0.9240\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 98s 347ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.4464 - val_accuracy: 0.9310\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 98s 349ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.5116 - val_accuracy: 0.9200\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 98s 346ms/step - loss: 0.0076 - accuracy: 0.9968 - val_loss: 0.4769 - val_accuracy: 0.9250\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 98s 347ms/step - loss: 0.0128 - accuracy: 0.9954 - val_loss: 0.4336 - val_accuracy: 0.9230\n",
      "Epoch 21/40\n",
      "282/282 [==============================] - 97s 345ms/step - loss: 0.0129 - accuracy: 0.9965 - val_loss: 0.4408 - val_accuracy: 0.9350\n",
      "Epoch 22/40\n",
      "282/282 [==============================] - 97s 345ms/step - loss: 8.8649e-04 - accuracy: 0.9999 - val_loss: 0.5137 - val_accuracy: 0.9310\n",
      "Epoch 23/40\n",
      "282/282 [==============================] - 97s 343ms/step - loss: 3.5159e-04 - accuracy: 1.0000 - val_loss: 0.5409 - val_accuracy: 0.9350\n",
      "Epoch 24/40\n",
      "282/282 [==============================] - 98s 346ms/step - loss: 2.2484e-04 - accuracy: 0.9999 - val_loss: 0.5550 - val_accuracy: 0.9340\n",
      "Epoch 25/40\n",
      "282/282 [==============================] - 96s 339ms/step - loss: 1.0985e-04 - accuracy: 1.0000 - val_loss: 0.5798 - val_accuracy: 0.9340\n",
      "Epoch 26/40\n",
      "282/282 [==============================] - 103s 364ms/step - loss: 9.0384e-05 - accuracy: 1.0000 - val_loss: 0.5977 - val_accuracy: 0.9350\n",
      "Epoch 27/40\n",
      "282/282 [==============================] - 104s 368ms/step - loss: 4.8838e-05 - accuracy: 1.0000 - val_loss: 0.6128 - val_accuracy: 0.9350\n",
      "Epoch 28/40\n",
      "282/282 [==============================] - 104s 370ms/step - loss: 4.2627e-05 - accuracy: 1.0000 - val_loss: 0.6229 - val_accuracy: 0.9360\n",
      "Epoch 29/40\n",
      "282/282 [==============================] - 104s 369ms/step - loss: 4.1292e-05 - accuracy: 1.0000 - val_loss: 0.6377 - val_accuracy: 0.9360\n",
      "Epoch 30/40\n",
      "282/282 [==============================] - 104s 368ms/step - loss: 8.1083e-05 - accuracy: 1.0000 - val_loss: 0.6492 - val_accuracy: 0.9330\n",
      "Epoch 31/40\n",
      "282/282 [==============================] - 104s 369ms/step - loss: 5.0095e-05 - accuracy: 1.0000 - val_loss: 0.6650 - val_accuracy: 0.9350\n",
      "Epoch 32/40\n",
      "282/282 [==============================] - 103s 366ms/step - loss: 2.0463e-05 - accuracy: 1.0000 - val_loss: 0.6841 - val_accuracy: 0.9340\n",
      "Epoch 33/40\n",
      "282/282 [==============================] - 104s 369ms/step - loss: 1.0884e-05 - accuracy: 1.0000 - val_loss: 0.6960 - val_accuracy: 0.9350\n",
      "Epoch 34/40\n",
      "282/282 [==============================] - 103s 365ms/step - loss: 1.2006e-05 - accuracy: 1.0000 - val_loss: 0.7093 - val_accuracy: 0.9340\n",
      "Epoch 35/40\n",
      "282/282 [==============================] - 103s 366ms/step - loss: 1.0451e-05 - accuracy: 1.0000 - val_loss: 0.7305 - val_accuracy: 0.9330\n",
      "Epoch 36/40\n",
      "282/282 [==============================] - 100s 356ms/step - loss: 9.0685e-06 - accuracy: 1.0000 - val_loss: 0.7416 - val_accuracy: 0.9320\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00036: early stopping\n",
      "Test Accuracy: 93.59999895095825\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 180s 563ms/step - loss: 0.3902 - accuracy: 0.8189 - val_loss: 0.3047 - val_accuracy: 0.8880\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 132s 470ms/step - loss: 0.1976 - accuracy: 0.9252 - val_loss: 0.2513 - val_accuracy: 0.9000\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 177s 628ms/step - loss: 0.1783 - accuracy: 0.9323 - val_loss: 0.2462 - val_accuracy: 0.9020\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 175s 622ms/step - loss: 0.1458 - accuracy: 0.9443 - val_loss: 0.2822 - val_accuracy: 0.9100\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 173s 613ms/step - loss: 0.1380 - accuracy: 0.9439 - val_loss: 0.2407 - val_accuracy: 0.9130\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 168s 594ms/step - loss: 0.1060 - accuracy: 0.9588 - val_loss: 0.2684 - val_accuracy: 0.9140\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 172s 608ms/step - loss: 0.0677 - accuracy: 0.9756 - val_loss: 0.3573 - val_accuracy: 0.9160\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 175s 621ms/step - loss: 0.0508 - accuracy: 0.9819 - val_loss: 0.3764 - val_accuracy: 0.9070\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 176s 625ms/step - loss: 0.0376 - accuracy: 0.9873 - val_loss: 0.3214 - val_accuracy: 0.9100\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 176s 623ms/step - loss: 0.0257 - accuracy: 0.9914 - val_loss: 0.4314 - val_accuracy: 0.9120\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 165s 585ms/step - loss: 0.0233 - accuracy: 0.9911 - val_loss: 0.4708 - val_accuracy: 0.9150\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 162s 576ms/step - loss: 0.0170 - accuracy: 0.9937 - val_loss: 0.5244 - val_accuracy: 0.9200\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 163s 577ms/step - loss: 0.0227 - accuracy: 0.9917 - val_loss: 0.4814 - val_accuracy: 0.9140\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 162s 576ms/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.7078 - val_accuracy: 0.9060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40\n",
      "282/282 [==============================] - 161s 571ms/step - loss: 0.0104 - accuracy: 0.9986 - val_loss: 0.6247 - val_accuracy: 0.8950\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 159s 563ms/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.5943 - val_accuracy: 0.9130\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 158s 559ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.6179 - val_accuracy: 0.9100\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 162s 576ms/step - loss: 0.0119 - accuracy: 0.9947 - val_loss: 0.6936 - val_accuracy: 0.9060\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 168s 597ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.6687 - val_accuracy: 0.9210\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 170s 602ms/step - loss: 0.0166 - accuracy: 0.9946 - val_loss: 0.5542 - val_accuracy: 0.9200\n",
      "Epoch 21/40\n",
      "282/282 [==============================] - 173s 612ms/step - loss: 0.0095 - accuracy: 0.9963 - val_loss: 0.6471 - val_accuracy: 0.9040\n",
      "Epoch 22/40\n",
      "282/282 [==============================] - 169s 601ms/step - loss: 0.0056 - accuracy: 0.9978 - val_loss: 0.6353 - val_accuracy: 0.9200\n",
      "Epoch 23/40\n",
      "282/282 [==============================] - 164s 582ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.7242 - val_accuracy: 0.9160\n",
      "Epoch 24/40\n",
      "282/282 [==============================] - 159s 564ms/step - loss: 2.2926e-04 - accuracy: 1.0000 - val_loss: 0.7699 - val_accuracy: 0.9220\n",
      "Epoch 25/40\n",
      "282/282 [==============================] - 158s 562ms/step - loss: 1.0486e-04 - accuracy: 1.0000 - val_loss: 0.7811 - val_accuracy: 0.9180\n",
      "Epoch 26/40\n",
      "282/282 [==============================] - 159s 564ms/step - loss: 7.4952e-05 - accuracy: 1.0000 - val_loss: 0.7984 - val_accuracy: 0.9200\n",
      "Epoch 27/40\n",
      "282/282 [==============================] - 158s 561ms/step - loss: 7.0423e-05 - accuracy: 1.0000 - val_loss: 0.8214 - val_accuracy: 0.9240\n",
      "Epoch 28/40\n",
      "282/282 [==============================] - 159s 563ms/step - loss: 6.1658e-05 - accuracy: 1.0000 - val_loss: 0.8505 - val_accuracy: 0.9200\n",
      "Epoch 29/40\n",
      "282/282 [==============================] - 156s 552ms/step - loss: 4.4269e-05 - accuracy: 1.0000 - val_loss: 0.8773 - val_accuracy: 0.9180\n",
      "Epoch 30/40\n",
      "282/282 [==============================] - 157s 557ms/step - loss: 3.5569e-05 - accuracy: 1.0000 - val_loss: 0.9002 - val_accuracy: 0.9220\n",
      "Epoch 31/40\n",
      "282/282 [==============================] - 156s 553ms/step - loss: 1.8244e-05 - accuracy: 1.0000 - val_loss: 0.9106 - val_accuracy: 0.9190\n",
      "Epoch 32/40\n",
      "282/282 [==============================] - 157s 557ms/step - loss: 2.5411e-05 - accuracy: 1.0000 - val_loss: 0.9343 - val_accuracy: 0.9220\n",
      "Epoch 33/40\n",
      "282/282 [==============================] - 157s 556ms/step - loss: 2.2608e-05 - accuracy: 1.0000 - val_loss: 0.9491 - val_accuracy: 0.9230\n",
      "Epoch 34/40\n",
      "282/282 [==============================] - 161s 571ms/step - loss: 2.2106e-05 - accuracy: 1.0000 - val_loss: 0.9604 - val_accuracy: 0.9230\n",
      "Epoch 35/40\n",
      "282/282 [==============================] - 168s 595ms/step - loss: 1.5727e-05 - accuracy: 1.0000 - val_loss: 0.9816 - val_accuracy: 0.9220\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00035: early stopping\n",
      "Test Accuracy: 92.40000247955322\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 139s 419ms/step - loss: 0.3930 - accuracy: 0.8286 - val_loss: 0.2406 - val_accuracy: 0.9010\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 118s 417ms/step - loss: 0.2199 - accuracy: 0.9182 - val_loss: 0.2126 - val_accuracy: 0.9110\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 120s 426ms/step - loss: 0.1845 - accuracy: 0.9313 - val_loss: 0.2139 - val_accuracy: 0.9110\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 122s 432ms/step - loss: 0.1494 - accuracy: 0.9461 - val_loss: 0.2352 - val_accuracy: 0.9010\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 124s 439ms/step - loss: 0.1195 - accuracy: 0.9535 - val_loss: 0.2559 - val_accuracy: 0.9130\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 124s 439ms/step - loss: 0.0948 - accuracy: 0.9620 - val_loss: 0.2837 - val_accuracy: 0.9090\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 124s 439ms/step - loss: 0.0713 - accuracy: 0.9742 - val_loss: 0.3109 - val_accuracy: 0.9040\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 124s 439ms/step - loss: 0.0479 - accuracy: 0.9811 - val_loss: 0.3423 - val_accuracy: 0.9110\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 124s 440ms/step - loss: 0.0416 - accuracy: 0.9856 - val_loss: 0.4599 - val_accuracy: 0.9060\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 132s 470ms/step - loss: 0.0253 - accuracy: 0.9890 - val_loss: 0.4658 - val_accuracy: 0.9120\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 132s 467ms/step - loss: 0.0189 - accuracy: 0.9937 - val_loss: 0.5474 - val_accuracy: 0.9090\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 125s 444ms/step - loss: 0.0278 - accuracy: 0.9904 - val_loss: 0.5206 - val_accuracy: 0.9120\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 121s 431ms/step - loss: 0.0100 - accuracy: 0.9964 - val_loss: 0.6185 - val_accuracy: 0.9070\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00013: early stopping\n",
      "Test Accuracy: 91.29999876022339\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 106s 314ms/step - loss: 0.3922 - accuracy: 0.8189 - val_loss: 0.2120 - val_accuracy: 0.9030\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 142s 503ms/step - loss: 0.2232 - accuracy: 0.9071 - val_loss: 0.1954 - val_accuracy: 0.9180\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 140s 496ms/step - loss: 0.1785 - accuracy: 0.9283 - val_loss: 0.2794 - val_accuracy: 0.8760\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 139s 492ms/step - loss: 0.1543 - accuracy: 0.9427 - val_loss: 0.1966 - val_accuracy: 0.9210\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 139s 492ms/step - loss: 0.1229 - accuracy: 0.9528 - val_loss: 0.2660 - val_accuracy: 0.9100\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 138s 489ms/step - loss: 0.1082 - accuracy: 0.9596 - val_loss: 0.2396 - val_accuracy: 0.9100\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 137s 486ms/step - loss: 0.0779 - accuracy: 0.9717 - val_loss: 0.2631 - val_accuracy: 0.9190\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 137s 486ms/step - loss: 0.0545 - accuracy: 0.9769 - val_loss: 0.3015 - val_accuracy: 0.9080\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 135s 480ms/step - loss: 0.0394 - accuracy: 0.9849 - val_loss: 0.3321 - val_accuracy: 0.9220\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 139s 493ms/step - loss: 0.0307 - accuracy: 0.9889 - val_loss: 0.3339 - val_accuracy: 0.9170\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 141s 501ms/step - loss: 0.0208 - accuracy: 0.9930 - val_loss: 0.3966 - val_accuracy: 0.9200\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 142s 503ms/step - loss: 0.0245 - accuracy: 0.9912 - val_loss: 0.4149 - val_accuracy: 0.9280\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 142s 503ms/step - loss: 0.0169 - accuracy: 0.9930 - val_loss: 0.4865 - val_accuracy: 0.9250\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 142s 502ms/step - loss: 0.0109 - accuracy: 0.9963 - val_loss: 0.4382 - val_accuracy: 0.9250\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 142s 502ms/step - loss: 0.0072 - accuracy: 0.9970 - val_loss: 0.4889 - val_accuracy: 0.9210\n",
      "Epoch 16/40\n",
      "282/282 [==============================] - 139s 492ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.4890 - val_accuracy: 0.9210\n",
      "Epoch 17/40\n",
      "282/282 [==============================] - 142s 505ms/step - loss: 0.0077 - accuracy: 0.9973 - val_loss: 0.5423 - val_accuracy: 0.9160\n",
      "Epoch 18/40\n",
      "282/282 [==============================] - 143s 506ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.5984 - val_accuracy: 0.9230\n",
      "Epoch 19/40\n",
      "282/282 [==============================] - 143s 506ms/step - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.5537 - val_accuracy: 0.9270\n",
      "Epoch 20/40\n",
      "282/282 [==============================] - 143s 506ms/step - loss: 0.0080 - accuracy: 0.9972 - val_loss: 0.4340 - val_accuracy: 0.9320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40\n",
      "282/282 [==============================] - 142s 505ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.5231 - val_accuracy: 0.9180\n",
      "Epoch 22/40\n",
      "282/282 [==============================] - 142s 504ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.5399 - val_accuracy: 0.9210\n",
      "Epoch 23/40\n",
      "282/282 [==============================] - 134s 476ms/step - loss: 0.0114 - accuracy: 0.9958 - val_loss: 0.5234 - val_accuracy: 0.9240\n",
      "Epoch 24/40\n",
      "282/282 [==============================] - 135s 480ms/step - loss: 0.0056 - accuracy: 0.9971 - val_loss: 0.6077 - val_accuracy: 0.9190\n",
      "Epoch 25/40\n",
      "282/282 [==============================] - 136s 483ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.5933 - val_accuracy: 0.9300\n",
      "Epoch 26/40\n",
      "282/282 [==============================] - 136s 483ms/step - loss: 2.1616e-04 - accuracy: 1.0000 - val_loss: 0.6158 - val_accuracy: 0.9240\n",
      "Epoch 27/40\n",
      "282/282 [==============================] - 137s 485ms/step - loss: 1.2342e-04 - accuracy: 1.0000 - val_loss: 0.6318 - val_accuracy: 0.9240\n",
      "Epoch 28/40\n",
      "282/282 [==============================] - 145s 516ms/step - loss: 9.5728e-05 - accuracy: 1.0000 - val_loss: 0.6532 - val_accuracy: 0.9240\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00028: early stopping\n",
      "Test Accuracy: 93.19999814033508\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 238s 764ms/step - loss: 0.3906 - accuracy: 0.8296 - val_loss: 0.2694 - val_accuracy: 0.8910\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 155s 550ms/step - loss: 0.2042 - accuracy: 0.9217 - val_loss: 0.2556 - val_accuracy: 0.8980\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 179s 636ms/step - loss: 0.1721 - accuracy: 0.9304 - val_loss: 0.2468 - val_accuracy: 0.9050\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 172s 608ms/step - loss: 0.1396 - accuracy: 0.9482 - val_loss: 0.2487 - val_accuracy: 0.9170\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 169s 601ms/step - loss: 0.1224 - accuracy: 0.9514 - val_loss: 0.3125 - val_accuracy: 0.9050\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 170s 602ms/step - loss: 0.1087 - accuracy: 0.9586 - val_loss: 0.3387 - val_accuracy: 0.9000\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 168s 597ms/step - loss: 0.0719 - accuracy: 0.9716 - val_loss: 0.3692 - val_accuracy: 0.9090\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 168s 596ms/step - loss: 0.0634 - accuracy: 0.9770 - val_loss: 0.3755 - val_accuracy: 0.9100\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 165s 586ms/step - loss: 0.0345 - accuracy: 0.9864 - val_loss: 0.4367 - val_accuracy: 0.9060\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 167s 592ms/step - loss: 0.0288 - accuracy: 0.9891 - val_loss: 0.5135 - val_accuracy: 0.8990\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 166s 588ms/step - loss: 0.0189 - accuracy: 0.9935 - val_loss: 0.5325 - val_accuracy: 0.9130\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 166s 590ms/step - loss: 0.0245 - accuracy: 0.9911 - val_loss: 0.5229 - val_accuracy: 0.8900\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 91.69999957084656\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 189s 617ms/step - loss: 0.3984 - accuracy: 0.8247 - val_loss: 0.2192 - val_accuracy: 0.9030\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 168s 595ms/step - loss: 0.2059 - accuracy: 0.9220 - val_loss: 0.2970 - val_accuracy: 0.8790\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 188s 668ms/step - loss: 0.1817 - accuracy: 0.9277 - val_loss: 0.2086 - val_accuracy: 0.9110\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 189s 671ms/step - loss: 0.1455 - accuracy: 0.9433 - val_loss: 0.2140 - val_accuracy: 0.9250\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 187s 664ms/step - loss: 0.1175 - accuracy: 0.9575 - val_loss: 0.2149 - val_accuracy: 0.9160\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 189s 669ms/step - loss: 0.0895 - accuracy: 0.9671 - val_loss: 0.2473 - val_accuracy: 0.9110\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 187s 664ms/step - loss: 0.0776 - accuracy: 0.9710 - val_loss: 0.2834 - val_accuracy: 0.9110\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 188s 667ms/step - loss: 0.0558 - accuracy: 0.9783 - val_loss: 0.3198 - val_accuracy: 0.9190\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 187s 663ms/step - loss: 0.0495 - accuracy: 0.9799 - val_loss: 0.3503 - val_accuracy: 0.9140\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 191s 677ms/step - loss: 0.0249 - accuracy: 0.9923 - val_loss: 0.4028 - val_accuracy: 0.9210\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 209s 743ms/step - loss: 0.0295 - accuracy: 0.9898 - val_loss: 0.4028 - val_accuracy: 0.9150\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 205s 727ms/step - loss: 0.0206 - accuracy: 0.9927 - val_loss: 0.4264 - val_accuracy: 0.9170\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00012: early stopping\n",
      "Test Accuracy: 92.5000011920929\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  91.000003  93.300003  90.899998  92.799997  93.599999  92.400002   \n",
      "\n",
      "        acc7       acc8  acc9      acc10    AVG  \n",
      "0  91.299999  93.199998  91.7  92.500001  92.27  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record2 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_2(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record2 = record2.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.000003</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>90.899998</td>\n",
       "      <td>92.799997</td>\n",
       "      <td>93.599999</td>\n",
       "      <td>92.400002</td>\n",
       "      <td>91.299999</td>\n",
       "      <td>93.199998</td>\n",
       "      <td>91.7</td>\n",
       "      <td>92.500001</td>\n",
       "      <td>92.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  91.000003  93.300003  90.899998  92.799997  93.599999  92.400002   \n",
       "\n",
       "        acc7       acc8  acc9      acc10    AVG  \n",
       "0  91.299999  93.199998  91.7  92.500001  92.27  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record2\n",
    "report = report.to_excel('GRU_SUBJ_v2_2.xlsx', sheet_name='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Word2Vec - Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this part,  we will fine tune the embeddings while training (dynamic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "def define_model_3(input_dim = None, output_dim=300, max_length = None, emb_matrix=None):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=input_dim, \n",
    "                                  mask_zero= True,\n",
    "                                  output_dim=output_dim, \n",
    "                                  input_length=max_length, \n",
    "                                  input_shape=(max_length, ),\n",
    "                                  # Assign the embedding weight with word2vec embedding marix\n",
    "                                  weights = [emb_matrix],\n",
    "                                  # Set the weight to be not trainable (static)\n",
    "                                  trainable = True),\n",
    "        \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 100, 300)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 100, 128)          140544    \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 128)               74496     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 523,361\n",
      "Trainable params: 523,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_0 = define_model_3( input_dim=1000, max_length=100, emb_matrix=np.random.rand(1000, 300))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # Overide the method on_epoch_end() for our benefit\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy') > 0.93):\n",
    "            print(\"\\nReached 93% accuracy so cancelling training!\")\n",
    "            self.model.stop_training=True\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                                             patience=8, verbose=2, \n",
    "                                             mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 307s 1s/step - loss: 0.3793 - accuracy: 0.8234 - val_loss: 0.1610 - val_accuracy: 0.9320\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 255s 906ms/step - loss: 0.0877 - accuracy: 0.9714 - val_loss: 0.1724 - val_accuracy: 0.9300\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 236s 838ms/step - loss: 0.0234 - accuracy: 0.9938 - val_loss: 0.2285 - val_accuracy: 0.9230\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 239s 849ms/step - loss: 0.0102 - accuracy: 0.9965 - val_loss: 0.4044 - val_accuracy: 0.9240\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 240s 851ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.4085 - val_accuracy: 0.9260\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 244s 864ms/step - loss: 5.8330e-04 - accuracy: 0.9998 - val_loss: 0.3744 - val_accuracy: 0.9240\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 251s 888ms/step - loss: 2.8277e-04 - accuracy: 1.0000 - val_loss: 0.5393 - val_accuracy: 0.9220\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 249s 882ms/step - loss: 1.7241e-04 - accuracy: 1.0000 - val_loss: 0.5636 - val_accuracy: 0.9260\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 242s 857ms/step - loss: 1.3903e-05 - accuracy: 1.0000 - val_loss: 0.5836 - val_accuracy: 0.9190\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 93.19999814033508\n",
      "Training 2: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 280s 938ms/step - loss: 0.3753 - accuracy: 0.8124 - val_loss: 0.2258 - val_accuracy: 0.9110\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 315s 1s/step - loss: 0.0878 - accuracy: 0.9722 - val_loss: 0.3501 - val_accuracy: 0.9060\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 322s 1s/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.4447 - val_accuracy: 0.8990\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 319s 1s/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.5428 - val_accuracy: 0.9110\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 329s 1s/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.4189 - val_accuracy: 0.9100\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 327s 1s/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.5923 - val_accuracy: 0.9090\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 329s 1s/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.7730 - val_accuracy: 0.8960\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 326s 1s/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.7454 - val_accuracy: 0.8990\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 323s 1s/step - loss: 7.8897e-05 - accuracy: 1.0000 - val_loss: 0.8251 - val_accuracy: 0.8960\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 91.10000133514404\n",
      "Training 3: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 279s 915ms/step - loss: 0.3645 - accuracy: 0.8377 - val_loss: 0.1849 - val_accuracy: 0.9310\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 210s 744ms/step - loss: 0.0814 - accuracy: 0.9722 - val_loss: 0.2039 - val_accuracy: 0.9270\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 218s 774ms/step - loss: 0.0229 - accuracy: 0.9934 - val_loss: 0.3638 - val_accuracy: 0.9050\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 214s 757ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.4634 - val_accuracy: 0.9060\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 212s 753ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.4462 - val_accuracy: 0.9100\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 218s 772ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.4297 - val_accuracy: 0.9190\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 215s 763ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.5132 - val_accuracy: 0.9170\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 213s 756ms/step - loss: 7.4692e-05 - accuracy: 1.0000 - val_loss: 0.5785 - val_accuracy: 0.9160\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 214s 758ms/step - loss: 3.1212e-05 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 0.9190\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 93.09999942779541\n",
      "Training 4: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 178s 575ms/step - loss: 0.3892 - accuracy: 0.8117 - val_loss: 0.1942 - val_accuracy: 0.9200\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 179s 634ms/step - loss: 0.0872 - accuracy: 0.9703 - val_loss: 0.2554 - val_accuracy: 0.9050\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 183s 648ms/step - loss: 0.0205 - accuracy: 0.9941 - val_loss: 0.2962 - val_accuracy: 0.9130\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 183s 648ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.4199 - val_accuracy: 0.9060\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 179s 636ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.5541 - val_accuracy: 0.9050\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 179s 635ms/step - loss: 0.0071 - accuracy: 0.9972 - val_loss: 0.4624 - val_accuracy: 0.9130\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 179s 635ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.4146 - val_accuracy: 0.9000\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 180s 637ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.5893 - val_accuracy: 0.9160\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 180s 637ms/step - loss: 0.0031 - accuracy: 0.9980 - val_loss: 0.5947 - val_accuracy: 0.9120\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 92.00000166893005\n",
      "Training 5: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 307s 1s/step - loss: 0.3689 - accuracy: 0.8200 - val_loss: 0.1781 - val_accuracy: 0.9330\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 279s 988ms/step - loss: 0.0710 - accuracy: 0.9759 - val_loss: 0.2425 - val_accuracy: 0.9110\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 267s 947ms/step - loss: 0.0250 - accuracy: 0.9921 - val_loss: 0.3009 - val_accuracy: 0.9190\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 269s 953ms/step - loss: 0.0062 - accuracy: 0.9977 - val_loss: 0.4130 - val_accuracy: 0.9300\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 278s 984ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.4294 - val_accuracy: 0.9210\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 272s 965ms/step - loss: 4.3951e-04 - accuracy: 1.0000 - val_loss: 0.7244 - val_accuracy: 0.9020\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 271s 960ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.5694 - val_accuracy: 0.9120\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 269s 956ms/step - loss: 0.0028 - accuracy: 0.9987 - val_loss: 0.4840 - val_accuracy: 0.9240\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 271s 962ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.5517 - val_accuracy: 0.9190\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 93.30000281333923\n",
      "Training 6: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 313s 1s/step - loss: 0.3743 - accuracy: 0.8219 - val_loss: 0.1930 - val_accuracy: 0.9250\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 332s 1s/step - loss: 0.0872 - accuracy: 0.9721 - val_loss: 0.2181 - val_accuracy: 0.9300\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 381s 1s/step - loss: 0.0175 - accuracy: 0.9948 - val_loss: 0.3475 - val_accuracy: 0.9080\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 383s 1s/step - loss: 0.0080 - accuracy: 0.9975 - val_loss: 0.3674 - val_accuracy: 0.9150\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 403s 1s/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.4998 - val_accuracy: 0.9190\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 426s 2s/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 0.5129 - val_accuracy: 0.9060\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 425s 2s/step - loss: 0.0035 - accuracy: 0.9985 - val_loss: 0.6262 - val_accuracy: 0.9140\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 427s 2s/step - loss: 6.1360e-04 - accuracy: 0.9998 - val_loss: 0.6428 - val_accuracy: 0.9120\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 427s 2s/step - loss: 2.7954e-04 - accuracy: 1.0000 - val_loss: 0.6861 - val_accuracy: 0.9130\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 425s 2s/step - loss: 2.4867e-05 - accuracy: 1.0000 - val_loss: 0.7131 - val_accuracy: 0.9140\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00010: early stopping\n",
      "Test Accuracy: 93.00000071525574\n",
      "Training 7: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 396s 1s/step - loss: 0.3740 - accuracy: 0.8213 - val_loss: 0.2059 - val_accuracy: 0.9180\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 279s 988ms/step - loss: 0.0878 - accuracy: 0.9720 - val_loss: 0.2478 - val_accuracy: 0.9220\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 276s 980ms/step - loss: 0.0207 - accuracy: 0.9928 - val_loss: 0.3533 - val_accuracy: 0.9100\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 281s 997ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.3993 - val_accuracy: 0.9100\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 263s 933ms/step - loss: 5.3629e-04 - accuracy: 0.9999 - val_loss: 0.6043 - val_accuracy: 0.9170\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 262s 929ms/step - loss: 5.0154e-04 - accuracy: 0.9997 - val_loss: 0.5811 - val_accuracy: 0.9230\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 264s 938ms/step - loss: 7.3273e-05 - accuracy: 1.0000 - val_loss: 0.6077 - val_accuracy: 0.9240\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 261s 926ms/step - loss: 2.2957e-05 - accuracy: 1.0000 - val_loss: 0.6349 - val_accuracy: 0.9230\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 263s 934ms/step - loss: 1.9896e-05 - accuracy: 1.0000 - val_loss: 0.6708 - val_accuracy: 0.9190\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 263s 932ms/step - loss: 1.3532e-05 - accuracy: 1.0000 - val_loss: 0.6924 - val_accuracy: 0.9190\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 263s 931ms/step - loss: 9.2499e-06 - accuracy: 1.0000 - val_loss: 0.7195 - val_accuracy: 0.9180\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 264s 938ms/step - loss: 8.5200e-06 - accuracy: 1.0000 - val_loss: 0.7367 - val_accuracy: 0.9200\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 263s 931ms/step - loss: 8.9273e-06 - accuracy: 1.0000 - val_loss: 0.7651 - val_accuracy: 0.9190\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 261s 927ms/step - loss: 5.7782e-06 - accuracy: 1.0000 - val_loss: 0.7863 - val_accuracy: 0.9190\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 261s 926ms/step - loss: 3.9832e-06 - accuracy: 1.0000 - val_loss: 0.8082 - val_accuracy: 0.9190\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "Test Accuracy: 92.40000247955322\n",
      "Training 8: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 363s 1s/step - loss: 0.3629 - accuracy: 0.8431 - val_loss: 0.1794 - val_accuracy: 0.9330\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 341s 1s/step - loss: 0.0816 - accuracy: 0.9738 - val_loss: 0.1945 - val_accuracy: 0.9320\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 351s 1s/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.2487 - val_accuracy: 0.9290\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 352s 1s/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.3643 - val_accuracy: 0.9180\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 362s 1s/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.5131 - val_accuracy: 0.9190\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 363s 1s/step - loss: 1.2692e-04 - accuracy: 1.0000 - val_loss: 0.5547 - val_accuracy: 0.9170\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 361s 1s/step - loss: 4.0631e-05 - accuracy: 1.0000 - val_loss: 0.5882 - val_accuracy: 0.9170\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 359s 1s/step - loss: 2.7483e-05 - accuracy: 1.0000 - val_loss: 0.6133 - val_accuracy: 0.9170\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 364s 1s/step - loss: 1.7146e-05 - accuracy: 1.0000 - val_loss: 0.6412 - val_accuracy: 0.9160\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 93.30000281333923\n",
      "Training 9: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 368s 1s/step - loss: 0.3832 - accuracy: 0.8264 - val_loss: 0.2152 - val_accuracy: 0.9180\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 400s 1s/step - loss: 0.0844 - accuracy: 0.9729 - val_loss: 0.2147 - val_accuracy: 0.9140\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 403s 1s/step - loss: 0.0183 - accuracy: 0.9950 - val_loss: 0.4708 - val_accuracy: 0.9050\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 380s 1s/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.4820 - val_accuracy: 0.8980\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 379s 1s/step - loss: 0.0071 - accuracy: 0.9975 - val_loss: 0.6312 - val_accuracy: 0.8980\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 379s 1s/step - loss: 0.0028 - accuracy: 0.9986 - val_loss: 0.5143 - val_accuracy: 0.9110\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 376s 1s/step - loss: 7.5922e-04 - accuracy: 0.9996 - val_loss: 0.7285 - val_accuracy: 0.9000\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 376s 1s/step - loss: 2.7545e-04 - accuracy: 1.0000 - val_loss: 0.7058 - val_accuracy: 0.9040\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 378s 1s/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.5099 - val_accuracy: 0.8840\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00009: early stopping\n",
      "Test Accuracy: 91.79999828338623\n",
      "Training 10: \n",
      "Epoch 1/40\n",
      "282/282 [==============================] - 290s 957ms/step - loss: 0.3713 - accuracy: 0.8307 - val_loss: 0.1719 - val_accuracy: 0.9320\n",
      "Epoch 2/40\n",
      "282/282 [==============================] - 248s 878ms/step - loss: 0.0772 - accuracy: 0.9731 - val_loss: 0.1840 - val_accuracy: 0.9310\n",
      "Epoch 3/40\n",
      "282/282 [==============================] - 222s 788ms/step - loss: 0.0245 - accuracy: 0.9935 - val_loss: 0.2756 - val_accuracy: 0.9210\n",
      "Epoch 4/40\n",
      "282/282 [==============================] - 223s 789ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.4955 - val_accuracy: 0.9090\n",
      "Epoch 5/40\n",
      "282/282 [==============================] - 224s 794ms/step - loss: 0.0048 - accuracy: 0.9980 - val_loss: 0.4605 - val_accuracy: 0.9340\n",
      "Epoch 6/40\n",
      "282/282 [==============================] - 229s 813ms/step - loss: 4.3843e-04 - accuracy: 1.0000 - val_loss: 0.5005 - val_accuracy: 0.9320\n",
      "Epoch 7/40\n",
      "282/282 [==============================] - 230s 817ms/step - loss: 4.8523e-05 - accuracy: 1.0000 - val_loss: 0.5219 - val_accuracy: 0.9370\n",
      "Epoch 8/40\n",
      "282/282 [==============================] - 229s 811ms/step - loss: 3.0571e-05 - accuracy: 1.0000 - val_loss: 0.5535 - val_accuracy: 0.9360\n",
      "Epoch 9/40\n",
      "282/282 [==============================] - 231s 818ms/step - loss: 2.8003e-05 - accuracy: 1.0000 - val_loss: 0.5697 - val_accuracy: 0.9340\n",
      "Epoch 10/40\n",
      "282/282 [==============================] - 232s 824ms/step - loss: 1.6368e-05 - accuracy: 1.0000 - val_loss: 0.5952 - val_accuracy: 0.9330\n",
      "Epoch 11/40\n",
      "282/282 [==============================] - 230s 815ms/step - loss: 1.1094e-05 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 0.9340\n",
      "Epoch 12/40\n",
      "282/282 [==============================] - 231s 820ms/step - loss: 5.5046e-06 - accuracy: 1.0000 - val_loss: 0.6282 - val_accuracy: 0.9340\n",
      "Epoch 13/40\n",
      "282/282 [==============================] - 230s 816ms/step - loss: 1.7242e-05 - accuracy: 1.0000 - val_loss: 0.6506 - val_accuracy: 0.9340\n",
      "Epoch 14/40\n",
      "282/282 [==============================] - 229s 812ms/step - loss: 3.4838e-06 - accuracy: 1.0000 - val_loss: 0.6672 - val_accuracy: 0.9340\n",
      "Epoch 15/40\n",
      "282/282 [==============================] - 230s 814ms/step - loss: 2.4567e-06 - accuracy: 1.0000 - val_loss: 0.6785 - val_accuracy: 0.9340\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.69999766349792\n",
      "\n",
      "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
      "0  93.199998  91.100001  93.099999  92.000002  93.300003  93.000001   \n",
      "\n",
      "        acc7       acc8       acc9      acc10        AVG  \n",
      "0  92.400002  93.300003  91.799998  93.699998  92.690001  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter Initialization\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<UNK>\"\n",
    "emb_mean = emb_mean\n",
    "emb_std = emb_std\n",
    "columns = ['acc1', 'acc2', 'acc3', 'acc4', 'acc5', 'acc6', 'acc7', 'acc8', 'acc9', 'acc10', 'AVG']\n",
    "record3 = pd.DataFrame(columns = columns)\n",
    "\n",
    "# prepare cross validation with 10 splits and shuffle = True\n",
    "kfold = KFold(10, True)\n",
    "\n",
    "# Separate the sentences and the labels\n",
    "sentences, labels = list(corpus.sentence), list(corpus.label)\n",
    "\n",
    "exp=0\n",
    "\n",
    "# kfold.split() will return set indices for each split\n",
    "acc_list = []\n",
    "for train, test in kfold.split(sentences):\n",
    "    \n",
    "    exp+=1\n",
    "    print('Training {}: '.format(exp))\n",
    "    \n",
    "    train_x, test_x = [], []\n",
    "    train_y, test_y = [], []\n",
    "\n",
    "    for i in train:\n",
    "        train_x.append(sentences[i])\n",
    "        train_y.append(labels[i])\n",
    "\n",
    "    for i in test:\n",
    "        test_x.append(sentences[i])\n",
    "        test_y.append(labels[i])\n",
    "\n",
    "    # Turn the labels into a numpy array\n",
    "    train_y = np.array(train_y)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # encode data using\n",
    "    # Cleaning and Tokenization\n",
    "    tokenizer = Tokenizer(oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "    # Turn the text into sequence\n",
    "    training_sequences = tokenizer.texts_to_sequences(train_x)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "    max_len = max_length(training_sequences)\n",
    "\n",
    "    # Pad the sequence to have the same size\n",
    "    Xtrain = pad_sequences(training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "    Xtest = pad_sequences(test_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index)+1\n",
    "    \n",
    "    emb_matrix = pretrained_embedding_matrix(word2vec, word_index, emb_mean, emb_std)\n",
    "\n",
    "    # Define the input shape\n",
    "    model = define_model_3(input_dim=vocab_size, max_length=max_len, emb_matrix=emb_matrix)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xtrain, train_y, batch_size=32, epochs=40, verbose=1, \n",
    "              callbacks=[callbacks], validation_data=(Xtest, test_y))\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n",
    "    print('Test Accuracy: {}'.format(acc*100))\n",
    "\n",
    "    acc_list.append(acc*100)\n",
    "\n",
    "mean_acc = np.array(acc_list).mean()\n",
    "entries = acc_list + [mean_acc]\n",
    "\n",
    "temp = pd.DataFrame([entries], columns=columns)\n",
    "record3 = record3.append(temp, ignore_index=True)\n",
    "print()\n",
    "print(record3)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc1</th>\n",
       "      <th>acc2</th>\n",
       "      <th>acc3</th>\n",
       "      <th>acc4</th>\n",
       "      <th>acc5</th>\n",
       "      <th>acc6</th>\n",
       "      <th>acc7</th>\n",
       "      <th>acc8</th>\n",
       "      <th>acc9</th>\n",
       "      <th>acc10</th>\n",
       "      <th>AVG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93.199998</td>\n",
       "      <td>91.100001</td>\n",
       "      <td>93.099999</td>\n",
       "      <td>92.000002</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>93.000001</td>\n",
       "      <td>92.400002</td>\n",
       "      <td>93.300003</td>\n",
       "      <td>91.799998</td>\n",
       "      <td>93.699998</td>\n",
       "      <td>92.690001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc1       acc2       acc3       acc4       acc5       acc6  \\\n",
       "0  93.199998  91.100001  93.099999  92.000002  93.300003  93.000001   \n",
       "\n",
       "        acc7       acc8       acc9      acc10        AVG  \n",
       "0  92.400002  93.300003  91.799998  93.699998  92.690001  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = record3\n",
    "report = report.to_excel('GRU_SUBJ_v2_3.xlsx', sheet_name='dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
